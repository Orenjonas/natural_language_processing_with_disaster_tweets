{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f65e5e75af0>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More readable tracebacks from the wonderful `rich` package\n",
    "from rich.traceback import install\n",
    "install(show_locals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Data processing\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Other\n",
    "from tqdm import tqdm  # Progress bar\n",
    "from IPython.display import display, Markdown  # For printing markdown formatted output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Saving objects\n",
    "We create some reuseable code for saving objects for later use, so we don't have to re-run time consuming code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('./input/nlp-getting-started/train.csv')\n",
    "test  = pd.read_csv('./input/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n",
      "(3263, 4)\n"
     ]
    }
   ],
   "source": [
    "print(tweet.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data rows contain a `keyword`, a `location` (sometimes not present), `text` containg a tweet and a `target` coding 1 for disaster and 0 for non-disaster.\n",
    "\n",
    "The training data contains 7613 observations, while the test data contains 3263 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>281</td>\n",
       "      <td>ambulance</td>\n",
       "      <td>VISIT MY YOUTUBE CHANNEL.</td>\n",
       "      <td>HAPPENING NOW - HATZOLAH EMS AMBULANCE RESPOND...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>283</td>\n",
       "      <td>ambulance</td>\n",
       "      <td>Lexington</td>\n",
       "      <td>http://t.co/FueRk0gWui Twelve feared killed in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>285</td>\n",
       "      <td>ambulance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://t.co/X5YEUYLT1X Twelve feared killed in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    keyword                   location  \\\n",
       "200  281  ambulance  VISIT MY YOUTUBE CHANNEL.   \n",
       "201  283  ambulance                  Lexington   \n",
       "202  285  ambulance                        NaN   \n",
       "\n",
       "                                                  text  target  \n",
       "200  HAPPENING NOW - HATZOLAH EMS AMBULANCE RESPOND...       0  \n",
       "201  http://t.co/FueRk0gWui Twelve feared killed in...       1  \n",
       "202  http://t.co/X5YEUYLT1X Twelve feared killed in...       1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.iloc[200:203]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance location does not seem trustworthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting a few tweets we see that we need to clean the text before we can analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Target: 0 -- https://t.co/WKv8VqVkT6 #ArtisteOfTheWeekFact say #Conversations by #coast2coastdjs agree @Crystal_Blaz 's #Jiwonle is a #HipHop #ClubBanger"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 1 -- RSS: Russia begins mass destruction of illegally imported food   http://t.co/r6JDj9kIGm"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 1 -- Emergency Flow  http://t.co/lH9mrYpDrJ mp3 http://t.co/PqhuthSS3i rar http://t.co/0iW6dRf5X9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 1 -- 8/6/2015@2:09 PM: TRAFFIC ACCIDENT NO INJURY at 2781 WILLIS FOREMAN RD http://t.co/VCkIT6EDEv"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 0 -- Absurdly Ridiculous MenÛªs #Fashion To Demolish You #Manhood. http://t.co/vTP8i8QLEn"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 0 -- i miss my longer hair..but it was so dead anyways it wasn't even hair"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 0 -- @Babybackreeve FATALITY!!!!!!!!!!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 1 -- Demolition Means Progress: Flint Michigan and the Fate of the American Metropolis Highsmith https://t.co/ZvoBMDxHGP"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 1 -- How do you derail a train at... Smithsonian?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 0 -- @paddytomlinson1 ARMAGEDDON"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [677, 2643, 3134, 92, 2290, 2062, 3681, 2343, 2384, 323]:\n",
    "    # Print using markdown for better formatting\n",
    "    tg = tweet.iloc[i][\"target\"]\n",
    "    tw = tweet.iloc[i][\"text\"]\n",
    "    display(Markdown(f\"Target: {tg} -- {tw}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that\n",
    "- There are many unusual symbols, such as in \"MenÛªs\", and hashtags (#)\n",
    "- We need to remove urls\n",
    "- Many tweets have date tags, such as \"8/6/2015@2:09 PM:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "The keyword column contain an important keyword present in the tweet, such as \"sinking\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>Do you feel like you are sinking in low self-i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>After a Few Years Afloat Pension Plans Start S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>Do you feel like you are sinking in unhappines...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>With a sinking music video tv career Brooke Ho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6089</th>\n",
       "      <td>@supernovalester I feel so bad for them. I can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>#nowplaying Sinking Fast - Now or Never on Nor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091</th>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6092</th>\n",
       "      <td>Nigga car sinking but he snapping it up for fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6093</th>\n",
       "      <td>@abandonedpics You should delete this one it's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "6085  Do you feel like you are sinking in low self-i...       0\n",
       "6086  After a Few Years Afloat Pension Plans Start S...       1\n",
       "6087  Do you feel like you are sinking in unhappines...       0\n",
       "6088  With a sinking music video tv career Brooke Ho...       0\n",
       "6089  @supernovalester I feel so bad for them. I can...       0\n",
       "6090  #nowplaying Sinking Fast - Now or Never on Nor...       0\n",
       "6091  that horrible sinking feeling when youÛªve be...       1\n",
       "6092  Nigga car sinking but he snapping it up for fo...       0\n",
       "6093  @abandonedpics You should delete this one it's...       0\n",
       "6094  that horrible sinking feeling when youÛªve be...       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet[tweet['keyword']=='sinking'][[\"text\", \"target\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like I found a contradiction in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-Target:  1\n",
      "-Tweet: \n",
      " that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\n",
      "\n",
      "-Target:  0\n",
      "-Tweet: \n",
      " that horrible sinking feeling when youÛªve been at home on your phone for a while and you realise its been on 3G this whole time\n"
     ]
    }
   ],
   "source": [
    "for i in [6091, 6094]:\n",
    "    print(\"\\n-Target: \",tweet.iloc[i].target)\n",
    "    print(\"-Tweet: \\n\", tweet.iloc[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- Inspect if there are many of these\n",
    "  - Find number of duplicate tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          7613\n",
       "keyword      221\n",
       "location    3341\n",
       "text        7503\n",
       "target         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_dist = tweet.target.value_counts()\n",
    "\n",
    "sns.barplot(x=value_dist.index, y=value_dist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tw_len = tweet.text.str.len()\n",
    "\n",
    "dta = {k:v for (k,v) in zip([\"Disaster\", \"Non-disaster\"], [tw_len[tweet.target==i] for i in [0,1]])}\n",
    "sns.histplot(dta)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "We will try adding a sentiment analysis score to our tweets. `SentimentIntensityAnalyzer` from `nltk` gives pieces of text a sentiment score between -1 and 1, where 1 is very positive and -1 is very negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add estimated sentiment to model input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "sia_table = []\n",
    "for tweet_i in tweet['text']:\n",
    "    sia_table.append(sia.polarity_scores(tweet_i)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['sentiment'] = sia_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We inspect ten random tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target | Sentiment | Tweet\n",
      "     1 |    0.0000 | http://t.co/iXiYBAp8Qa The Latest: More homes razed by Northern California wildfire - Lynchburg News and Advance http://t.co/zEpzQYDby4\n",
      "     0 |    0.6636 | Reading for work has collided with reading for pleasure. Huzzah. Don't miss @molly_the_tanz's Vermilion! http://t.co/83bMprwH7W\n",
      "     0 |    0.0000 | think i'll become a businessman a demolish a community centre and build condos on it but foiled by a troupe of multi-racial breakdancers .\n",
      "     1 |   -0.8481 | 70 Years After Atomic Bombs Japan Still Struggles With War Past: The anniversary of the devastation wrought b... http://t.co/LtVVPfLSg8\n",
      "     0 |    0.0000 | 'Snowstorm' 36'x36' oil on canvas (2009) http://t.co/RCZAlRU05o #art #painting\n",
      "     0 |    0.0000 | We would become the mirrors that reflected each other's most private wounds and desires.\n",
      "     1 |   -0.6369 | #Colorado #News Motorcyclist bicyclist injured in Denver collision on Broadway: At least two people were tak... http://t.co/2iAFPmqJeP\n",
      "     1 |   -0.6249 | Families to sue over Legionnaires: More than 40 families affected by the fatal outbreak of Legionnaires' disea... http://t.co/NQ77EfMF88\n",
      "     1 |    0.0000 | VIDEO: 'We're picking up bodies from water': Rescuers are searching for hundreds of migrants in the Mediterran... http://t.co/ZFWMjh6SLh\n",
      "     0 |   -0.6808 | @punkblunts @sincerelyevelnn fall off a cliff into hell idc\n"
     ]
    }
   ],
   "source": [
    "print(f\"Target | Sentiment | Tweet\")\n",
    "for i, row in tweet.iloc[[5585, 1708, 2297, 2759, 6229, 7439, 1766, 5314, 5725, 1582]].iterrows():\n",
    "    print(f\"{row.target:6} | {row.sentiment:9.4f} | {row.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x=tweet.target, y=tweet.sentiment)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment of disaster tweets seem to fall slightly lower than non-disaster tweets on average. It might have stronger predictive quality together with high leverl features of the tweets discovered by the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment does not seem to separate the two classes, but might be predictive in connection with higher level features detected by the neural network.\n",
    "- TODO: How to implement sentiment in the analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram analysis\n",
    "- Uninformative\n",
    "- From https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove#Exploratory-Data-Analysis-of-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tweet_bigrams(corpus, n=None):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding vectorization\n",
    "We will vectorize words using a library of vectors from a pre trained model.\n",
    "\n",
    "- TODO: Fine tune on current dataset\n",
    "  - see demo.sh in the github repo\n",
    "- Alternative dataset: https://allennlp.org/elmo\n",
    "\n",
    "\n",
    "We will use GloVe for vectorization of words found at https://github.com/stanfordnlp/GloVe,\n",
    "trying the twitter dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mmap\n",
    "\n",
    "def get_num_lines(file_path: str):\n",
    "    \"\"\"\n",
    "    Get the number of lines in a file. \n",
    "    Used in the tqdm module to get a progress bar for \n",
    "    for-loops when iterating over lines in a file.\n",
    "    \"\"\"\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this once to save an embedding dictionary to a file in and obj/ folder in current dir (`obj` dir must be created beforehand)\n",
    "\n",
    "# embedding_dict={}\n",
    "# file_path = './input/glove.twitter.27B.100d.txt'\n",
    "# with open(file_path,'r') as f:\n",
    "#     for line in tqdm(f, total = get_num_lines(file_path)):\n",
    "#         values=line.split()\n",
    "#         word=values[0]\n",
    "#         vectors=np.asarray(values[1:],'float32')\n",
    "#         embedding_dict[word]=vectors\n",
    "# f.close()\n",
    "\n",
    "# save_obj(embedding_dict, \"embedding_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved embedding dictionary from previous cell\n",
    "embedding_dict = load_obj(\"embedding_dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO: Visualize embeddings with PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to preprocess the data based on how words are embedded into the pre-trained embeddings.\n",
    "\n",
    "We will first attempt some boiler-plate text cleaning, courtesy of among others [this notebook](https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove#GloVe-for-Vectorization), and inspect how well it coincides with the embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inital cleaning attempt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mortal Kombat X: All Fatalities On Meat Predator.\\nhttps://t.co/IggFNBIxt5'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.text.iloc[3639]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame):\n",
    "    import string\n",
    "    import re\n",
    "\n",
    "    punctuation_regex = re.compile(f\"[{re.escape(string.punctuation)}:]\")\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"\\U00002702-\\U000027B0\"  # Symbols\n",
    "                               \"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    # Clean training data\n",
    "    df.keyword.str.replace(\"%20\", \" \")  # We replace ascii %20 with space in the keywords, e.g 'body%20bags' -> 'body bags'\n",
    "    df.text = df.text.str.lower()\n",
    "    df.text = df.text.str.replace(r\"https[^\\s|$]*\", \"\", regex=True)  # Change all urls to \"URL\"\n",
    "    df.text = df.text.str.replace(punctuation_regex, \"\", regex=True)\n",
    "    df.text = df.text.str.replace(emoji_pattern, \"\", regex=True)  # Remove emojis and symbols\n",
    "    df.text = df.text.str.replace(r\"\\n\", \" \", regex = True)       # Change \\n to space\n",
    "    df.text = df.text.str.replace(r\"[^a-zA-Z0-9 ]\", \"\", regex=True)  # Remove last non word characters\n",
    "    df.text = df.text.str.replace(r\"<.*?>\", \"\", regex=True)  # Remove html tags (e.g. <div> )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet = clean_data(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mortal kombat x all fatalities on meat predator '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tweet.text.iloc[3639]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check embedding coverage\n",
    "\n",
    "We check how many of the words in our training set tweets are covered by the embedding dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_representation(word_dict: dict , word_list: list):\n",
    "    \"\"\"Return the words from uq_words not contained in word_dict \n",
    "    and the number of words not covered.\"\"\"\n",
    "    n_covered = 0\n",
    "    not_covered = []\n",
    "    for word in word_list:\n",
    "        if word in word_dict:\n",
    "            n_covered += 1\n",
    "        else:\n",
    "            not_covered.append(word)\n",
    "\n",
    "    return n_covered, not_covered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unused_words(word_dict: dict , word_list: list):\n",
    "    \"\"\"Returns a list of words from word_dict not contained in word_list\"\"\"\n",
    "    unused_words = word_dict.copy()#.keys())\n",
    "\n",
    "    # Remove words in tweet data from the dict\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            unused_words.pop(word)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    #Convert dict to list\n",
    "    return list(unused_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_words = tweet.text.str.split(expand=True).stack().unique()\n",
    "n_covered, not_covered = word_representation(word_dict = embedding_dict, word_list = uq_words)\n",
    "unused_words = get_unused_words(word_dict = embedding_dict, word_list = uq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Emojis have an embedding and might encode and important meaning in the tweets\n",
    "- There seems to be <user>, <hashtag>, <url> and <number> placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the coverage of the tweets, we see that only 56% of the words in our data are in the embedding dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5676696863477034"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_covered/len(uq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While only 1% of the words in the embedding dictionary is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01044813010309883"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_covered/len(embedding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calls for further inquiry. Checking the words not covered by the embeddings, we see that there are numbers, URL's and  words with repeated number of letters (elongated words) eg. 'goooooooaaaaaal' among other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13000,  rockyfire,  20,  cafire,  18,  19,  80,  goooooooaaaaaal,  bbcmtd,  httptcolhyxeohy6c,  httptcoyao1e0xngw,  africanbaze,  newsnigeria,  httptco2nndbgwyei,  httptcoqqsmshaj3n,  phdsquares,  httptco3imaomknna,  superintende,  httptcowdueaj8q4j,  httptcoroi2nsmejj'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",  \".join(not_covered[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When inspecting some of the unused words in the embedding dictionary we see that many things, such as hashtags, repeated letters in words, allcaps words and smileys are encoded with special placeholders, such as <allcaps>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<user>  .  :  ,  <repeat>  <hashtag>  <number>  <url>  !  \"  ?  (  <allcaps>  <elong>  )  <smile>  ！  。  -  、  …  /  \\'s  *  n\\'t  \\'  \\'m  >  ^  ？  <  ・  &  ♥  lo  “  ”  por  _  <sadface>  من  ♡  ´  ،  ~  ;  <heart>  aku  \\'re  <lolface>  una  （  >>  في  ･  le  é  |  [  ）  ]  yg  —  笑  ω  je  yang  ❤  não  ～  ★  `  dia  $  و  الله  pero  ♪  \\'ll  =  nn  ｀  ¿  <neutralface>  لا  +  ada  ☆  ni  \\'ve  itu  على  -_-  ☺  ما  todo  mais  ini  ﾟ  aja'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"  \".join(unused_words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and pre-processing for the GloVe embedding\n",
    "In their [info page](https://nlp.stanford.edu/projects/glove/) the writers of the GloVe algorithm supply a ruby regex used for text pre-processing for the twitter model.\n",
    "\n",
    "In an [issue thread](https://github.com/stanfordnlp/GloVe/issues/107) discussing text pre-processing for tweets on their Github page user [skondrashov](https://github.co/skondrashov) supplies a useful python conversion of this ruby script, that also illustrates how words are adjusted to fit in a standard dictionary, and tagged for special characters or rewritings, such as being prefixed with a hashtag or elongated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove contractions\n",
    "Copied from https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/, with a small tweak for not matching `'s` in words surrounded by single quotation mark, like `'sylvester stallone'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r\"(ain't|(?<=[a-zA-Z])'s|aren't|can't|can't've|'cause|could've|couldn't|couldn't've|didn't|doesn't|don't|hadn't|hadn't've|hasn't|haven't|he'd|he'd've|he'll|he'll've|how'd|how'd'y|how'll|i'd|i'd've|i'll|i'll've|i'm|i've|isn't|it'd|it'd've|it'll|it'll've|let's|ma'am|mayn't|might've|mightn't|mightn't've|must've|mustn't|mustn't've|needn't|needn't've|o'clock|oughtn't|oughtn't've|shan't|sha'n't|shan't've|she'd|she'd've|she'll|she'll've|should've|shouldn't|shouldn't've|so've|that'd|that'd've|there'd|there'd've|they'd|they'd've|they'll|they'll've|they're|they've|to've|wasn't|we'd|we'd've|we'll|we'll've|we're|we've|weren't|what'll|what'll've|what're|what've|when've|where'd|where've|who'll|who'll've|who've|why've|will've|won't|won't've|would've|wouldn't|wouldn't've|y'all|y'all'd|y'all'd've|y'all're|y'all've|you'd|you'd've|you'll|you'll've|you're|you've)\",\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Dictionary of English Contractions\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"i'd\": \"i would\", \"i'd've\": \"i would have\",\"i'll\": \"i will\",\n",
    "                     \"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\"}\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "#    adding positive lookbehind for `'s` in the regex to make sure a letter is preceeding\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()).lower().replace(\"|'s\", \"|(?<=[a-zA-Z])'s\"))\n",
    "contractions_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet= pd.read_csv('./input/nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('./input/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for expanding contractions\n",
    "# def expand_contractions(text, contractions_dict=contractions_dict):\n",
    "#     for match in contractions_re.findall(text):\n",
    "#     return contractions_re.sub(replace, text)\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(1)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "# Expanding Contractions in the reviews\n",
    "df = tweet.copy()\n",
    "df.loc[:, 'text']=df.loc[:, 'text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's an emergency evacuation happening now in the building across the street\n",
      "There is an emergency evacuation happening now in the building across the street\n",
      "What's up man?\n",
      "What is up man?\n",
      "No way...I can't eat that shit\n",
      "No way...I cannot eat that shit\n",
      "@PhDSquares #mufc they've built so much hype around new acquisitions but I doubt they will set the EPL ablaze this season.\n",
      "@PhDSquares #mufc they have built so much hype around new acquisitions but I doubt they will set the EPL ablaze this season.\n",
      "on the outside you're ablaze and alive\n",
      "but you're dead inside\n",
      "on the outside you are ablaze and alive\n",
      "but you are dead inside\n",
      "First night with retainers in. It's quite weird. Better get used to it; I have to wear them every single night for the next year at least.\n",
      "First night with retainers in. It is quite weird. Better get used to it; I have to wear them every single night for the next year at least.\n",
      "@ablaze what time does your talk go until? I don't know if I can make it due to work.\n",
      "@ablaze what time does your talk go until? I do not know if I can make it due to work.\n",
      "'I can't have kids cuz I got in a bicycle accident &amp; split my testicles. it's impossible for me to have kids' MICHAEL YOU ARE THE FATHER\n",
      "'I cannot have kids cuz I got in a bicycle accident &amp; split my testicles. it is impossible for me to have kids' MICHAEL YOU ARE THE FATHER\n",
      "mom: 'we didn't get home as fast as we wished' \n",
      "me: 'why is that?'\n",
      "mom: 'there was an accident and some truck spilt mayonnaise all over ??????\n",
      "mom: 'we did not get home as fast as we wished' \n",
      "me: 'why is that?'\n",
      "mom: 'there was an accident and some truck spilt mayonnaise all over ??????\n",
      "#TruckCrash Overturns On #FortWorth Interstate http://t.co/Rs22LJ4qFp Click here if you've been in a crash&gt;http://t.co/Ld0unIYw4k\n",
      "#TruckCrash Overturns On #FortWorth Interstate http://t.co/Rs22LJ4qFp Click here if you have been in a crash&gt;http://t.co/Ld0unIYw4k\n"
     ]
    }
   ],
   "source": [
    "# Print the ten first edited tweets\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "while j < 10:\n",
    "    a = tweet.loc[i,'text']\n",
    "    b = df.loc[i,'text']\n",
    "    if (len(a) != len(b)):\n",
    "        print(a)\n",
    "        print(b)\n",
    "        j += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a cleaning function for repeated letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g', 'o', 'oo', ''), ('', 'a', 'aaa', ''), ('', 'l', 'll', '')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: \n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "word = \"goooaaaallls\"  # We want to match go[oo]a[aaa]l[ll]s\n",
    "\n",
    "# Get list of matches: [(group1, group2, ...), ...] where match group 3 is the repeated letters\n",
    "m = re.findall(r\"(\\S*?)(\\w)(\\2{1,})(\\S*?)\", word)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oo', 'aaa', 'll']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_letters = [match[2] for match in m]\n",
    "repeated_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned word - Removed letters\n",
      "=============================\n",
      "goals         ('oo', 'aaa', 'll')\n",
      "goallls       ('oo', 'aaa')\n",
      "goaaaals      ('oo', 'll')\n",
      "goooals       ('aaa', 'll')\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Loop over all combinations of repeated letters\n",
    "print(f\"{'Cleaned word':12s} - Removed letters\")\n",
    "print(\"=============================\")\n",
    "for i in range(len(repeated_letters), 0, -1):\n",
    "    for combination in combinations(repeated_letters, r=i+1):\n",
    "        tword = word\n",
    "        for letters in combination:\n",
    "            tword = re.sub(letters, \"\", tword)\n",
    "        print(f\"{tword:14s}{combination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning function for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def clean_tweets(df):\n",
    "    import re\n",
    "\n",
    "    def sub(pattern, output, string, whole_word=False):\n",
    "        token = output\n",
    "        if whole_word:\n",
    "            pattern = r'(\\s|^)' + pattern + r'(\\s|$)'\n",
    "\n",
    "        if isinstance(output, str):\n",
    "            token = ' ' + output + ' '\n",
    "        else:\n",
    "            token = lambda match: ' ' + output(match) + ' '\n",
    "\n",
    "        return re.sub(pattern, token, string)\n",
    "\n",
    "\n",
    "    def hashtag(token):\n",
    "        \"\"\" Replace hashtag `#` with `<hashtag>` and split following joined words.\"\"\"\n",
    "        token = token.group('tag')\n",
    "        if token != token.upper():\n",
    "            token = ' '.join(re.findall('[a-zA-Z][^A-Z]*', token))\n",
    "\n",
    "        return '<hashtag> ' + token\n",
    "\n",
    "    def punc_repeat(token):\n",
    "        return token.group(0)[0] + \" <repeat>\"\n",
    "\n",
    "    def punc_separate(token):\n",
    "        return token.group()\n",
    "\n",
    "    def number(token):\n",
    "        return token.group() + ' <number>';\n",
    "\n",
    "    def word_end_repeat(token):\n",
    "        return token.group(1) + token.group(2) + ' <elong>'\n",
    "    \n",
    "    def allcaps(token):\n",
    "        return token.group() + ' <allcaps>'\n",
    "\n",
    "    def clean_repeated_letters(tweet: str, embedding_dict: dict):\n",
    "        \"\"\"\n",
    "        Splits a tweet into words, finds repeated letters in the word and\n",
    "        removes combinations of the repeated letters until the word is matched by a key in\n",
    "        embedding_dict\n",
    "        \"\"\"\n",
    "\n",
    "        cleaned_tweet = []\n",
    "\n",
    "        for word_i in tweet.split():\n",
    "            word_found = False\n",
    "            if word_i in embedding_dict:\n",
    "                cleaned_tweet.append(word_i)\n",
    "                continue\n",
    "\n",
    "            matches = re.findall(r\"\"\"(\\S*?)    # 1: Optional preceeding letters\n",
    "                                     (\\w)      # 2: A letter that might be repeated\n",
    "                                     (\\2{1,})  # 3: Repetead instances of the preceeding letter (group 2)\n",
    "                                     (\\S*?)    # 4: Optional trailing letters\"\"\",\n",
    "                                 word_i,\n",
    "                                 flags=re.X)  # Verbose regex, for commenting\n",
    "                                 \n",
    "            repeated_letters = [match[2] for match in matches]\n",
    "                    \n",
    "            # Loop over all combinations of repeated letters\n",
    "            for i in range(len(repeated_letters), 0, -1):  # i decides length of combination\n",
    "                if word_found:\n",
    "                    continue\n",
    "                    \n",
    "                for combination in combinations(repeated_letters, r = i):\n",
    "                    if word_found:\n",
    "                        continue\n",
    "                        \n",
    "                    tword = word_i \n",
    "                    \n",
    "                        \n",
    "                    for letters in combination:\n",
    "                        tword = re.sub(letters, \"\", tword, count=1)\n",
    "                                        \n",
    "                        # Word in the embedding dict?\n",
    "                        if (tword in embedding_dict):\n",
    "                            # Keep the word and stop searching\n",
    "                            word_found = True\n",
    "                            tword = tword + \" <elong>\"\n",
    "                            continue  \n",
    "            if not word_found:\n",
    "                # No match, we simply keep the word\n",
    "                tword = word_i\n",
    "                \n",
    "            cleaned_tweet.append(tword)\n",
    "            \n",
    "        return \" \".join(cleaned_tweet)\n",
    "\n",
    "\n",
    "\n",
    "    eyes        = r\"[8:=;]\"\n",
    "    nose        = r\"['`\\-\\^]?\"\n",
    "    sad_front   = r\"[(\\[/\\\\]+\"\n",
    "    sad_back    = r\"[)\\]/\\\\]+\"\n",
    "    smile_front = r\"[)\\]]+\"\n",
    "    smile_back  = r\"[(\\[]+\"\n",
    "    lol_front   = r\"[DbpP]+\"\n",
    "    lol_back    = r\"[d]+\"\n",
    "    neutral     = r\"[|]+\"\n",
    "    sadface     = eyes + nose + sad_front   + '|' + sad_back   + nose + eyes\n",
    "    smile       = eyes + nose + smile_front + '|' + smile_back + nose + eyes\n",
    "    lolface     = eyes + nose + lol_front   + '|' + lol_back   + nose + eyes\n",
    "    neutralface = eyes + nose + neutral     + '|' + neutral    + nose + eyes\n",
    "    punctuation = r\"\"\"[ '!\"#$%&'()+,/:;=?@_`{|}~\\*\\-\\.\\^\\\\\\[\\]]+\"\"\" ## < and > omitted to avoid messing up tokens\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        df.loc[i,'text'] = sub(r'[\\s]+',                             '  ',            df.loc[i,'text']) # ensure 2 spaces between everything\n",
    "        df.loc[i,'text'] = sub(r'(?:(?:https?|ftp)://|www\\.)[^\\s]+', '<url>',         df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(r'@\\w+',                              '<user>',        df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(r'#(?P<tag>\\w+)',                     hashtag,         df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(sadface,                              '<sadface>',     df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(smile,                                '<smile>',       df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(lolface,                              '<lolface>',     df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(neutralface,                          '<neutralface>', df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(r'(?:<3+)+',                          '<heart>',       df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(r'\\b[A-Z]+\\b',                         allcaps,       df.loc[i,'text'], True) \n",
    "        # Allcaps tag\n",
    "        df.loc[i,'text'] = df.loc[i,'text'].lower()\n",
    "        df.loc[i,'text'] = expand_contractions(df.loc[i, 'text'])\n",
    "        df.loc[i,'text'] = sub(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*',          number,          df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(punctuation,                          punc_separate,   df.loc[i,'text'])\n",
    "        df.loc[i,'text'] = sub(r'([!?.])\\1+',                        punc_repeat,     df.loc[i,'text'])\n",
    "#     df.loc[i,'text'] = sub(r'(\\S*?)(\\w)\\2+\\b',                   word_end_repeat, df.loc[i,'text'])\n",
    "        \n",
    "        df.loc[i,'text'] = clean_repeated_letters(df.loc[i,'text'], embedding_dict)\n",
    "#     tweet = sub(r\"(\\S*?)(\\w)(\\2{1,})(\\S*?)\",          word_repeat,     tweet)\n",
    "#     tweet = sub(r'(\\S*?)(\\w*(\\w)\\2+\\w*)\\2+\\b',                   word_repeat, tweet)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  I'm hoping they're helping, they've got to\n",
      "Cleaned:   i am hoping they are helping , they have got to\n",
      "Original:  goooooooaaaaaallll, hey its a goall gooal\n",
      "Cleaned:   goal <elong> , hey its a goall gooal\n",
      "Original:  http://foo.com/blah_blah http://foo.com/blah_blah/ http://foo.com/blah_blah_(wikipedia) https://foo_bar.example.com/\n",
      "Cleaned:   <url> <url> <url> <url>\n",
      "Original:  :\\ :-/ =-( =`( )'8 ]^; -.- :/\n",
      "Cleaned:   <sadface> <sadface> <sadface> <sadface> <sadface> <sadface> -.- <sadface>\n",
      "Original:  :) :-] =`) ('8 ;`)\n",
      "Cleaned:   <smile> <smile> <smile> <smile> <smile>\n",
      "Original:  :D :-D =`b d'8 ;`P\n",
      "Cleaned:   <lolface> <lolface> <lolface> <lolface> <lolface>\n",
      "Original:  :| 8|\n",
      "Cleaned:   <neutralface> <neutralface>\n",
      "Original:  <3<3 <3 <3\n",
      "Cleaned:   <heart> <heart> <heart>\n",
      "Original:  #swag #swa00-= #as ## #WOOP #Feeling_Blessed #helloWorld\n",
      "Cleaned:   <hashtag> swag # swa00 -= <hashtag> as ## <hashtag> woop <allcaps> <hashtag> feeling _ blessed <hashtag> hello world\n",
      "Original:  holy crap!! i won!!!!@@!!!\n",
      "Cleaned:   holy crap ! <repeat> i won ! <repeat> @@ ! <repeat>\n",
      "Original:  holy *IUYT$)(crap!! @@#i@%#@ swag.lord **won!!!!@@!!! wahoo....!!!??!??? Im sick lol.\n",
      "Cleaned:   holy * iuyt $)( crap ! <repeat> @@# i @%#@ swag . lord ** won ! <repeat> @@ ! <repeat> wahoo . <repeat> ! <repeat> ? <repeat> ! ? <repeat> im sick lol .\n",
      "Original:  this SENTENCE consisTS OF slAyYyyy slayyyyyy #WEIRD caPITalIZAtionn\n",
      "Cleaned:   this sentence <allcaps> consists of <allcaps> slay <elong> slay <elong> <hashtag> weird <allcaps> capitalization <elong>\n"
     ]
    }
   ],
   "source": [
    "temp = pd.DataFrame({\"text\": [\n",
    "    u\"I'm hoping they're helping, they've got to\",\n",
    "    u'goooooooaaaaaallll, hey its a goall gooal',\n",
    "    u'http://foo.com/blah_blah http://foo.com/blah_blah/ http://foo.com/blah_blah_(wikipedia) https://foo_bar.example.com/',\n",
    "    u':\\\\ :-/ =-( =`( )\\'8 ]^; -.- :/',\n",
    "    u':) :-] =`) (\\'8 ;`)',\n",
    "    u':D :-D =`b d\\'8 ;`P',\n",
    "    u':| 8|',\n",
    "    u'<3<3 <3 <3',\n",
    "    u'#swag #swa00-= #as ## #WOOP #Feeling_Blessed #helloWorld',\n",
    "    u'holy crap!! i won!!!!@@!!!',\n",
    "    u'holy *IUYT$)(crap!! @@#i@%#@ swag.lord **won!!!!@@!!! wahoo....!!!??!??? Im sick lol.',\n",
    "    u'this SENTENCE consisTS OF slAyYyyy slayyyyyy #WEIRD caPITalIZAtionn',\n",
    "    ]})\n",
    "temp_uncleaned = temp.copy()\n",
    "clean_tweets(df = temp)\n",
    "\n",
    "for i in range(temp.shape[0]):\n",
    "#     print(\"====================\")\n",
    "    print(\"Original: \", temp_uncleaned.iloc[i].text)\n",
    "    print(\"Cleaned:  \", temp.iloc[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `goall` and `gooal` is in the GloVe embedding dictionary, they are not shortened to `goal`.\n",
    "\n",
    "As they "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and checking embedding coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet= pd.read_csv('./input/nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('./input/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this &lt;hashtag&gt; ear...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask . canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to ' shelter in place ' ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 , 000 &lt;number&gt; people receive &lt;hashtag&gt; wil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby &lt;hashtag&gt; a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; the out of control wild fires in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>m1 . 94 [ 01 : 04 utc ]? 5km s &lt;allcaps&gt; of vo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>police investigating after an e - bike collide...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the latest : more homes razed by northern cali...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     our deeds are the reason of this <hashtag> ear...       1  \n",
       "1               forest fire near la ronge sask . canada       1  \n",
       "2     all residents asked to ' shelter in place ' ar...       1  \n",
       "3     13 , 000 <number> people receive <hashtag> wil...       1  \n",
       "4     just got sent this photo from ruby <hashtag> a...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  two giant cranes holding a bridge collapse int...       1  \n",
       "7609  <user> <user> the out of control wild fires in...       1  \n",
       "7610  m1 . 94 [ 01 : 04 utc ]? 5km s <allcaps> of vo...       1  \n",
       "7611  police investigating after an e - bike collide...       1  \n",
       "7612  the latest : more homes razed by northern cali...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweets(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this &lt;hashtag&gt; ear...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask . canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to ' shelter in place ' ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 , 000 &lt;number&gt; people receive &lt;hashtag&gt; wil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby &lt;hashtag&gt; a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  our deeds are the reason of this <hashtag> ear...   \n",
       "1   4     NaN      NaN            forest fire near la ronge sask . canada   \n",
       "2   5     NaN      NaN  all residents asked to ' shelter in place ' ar...   \n",
       "3   6     NaN      NaN  13 , 000 <number> people receive <hashtag> wil...   \n",
       "4   7     NaN      NaN  just got sent this photo from ruby <hashtag> a...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_words = tweet.text.str.split(expand=True).stack().unique()\n",
    "n_covered, not_covered = word_representation(word_dict = embedding_dict, word_list = uq_words)\n",
    "unused_words = get_unused_words(word_dict = embedding_dict, word_list = uq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now 82% of the words in our data are in the embedding dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8232293505983912"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_covered/len(uq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still only 1% of the words in the embedding dictionary is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01054699773358525"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_covered/len(embedding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words not covered by the embedding dictionary seem to be joined words such as `myreligion` many uncommon symbols and words and numbers. Hopefully these do not carry much meaning, and as they are uncommon it will be hard for our model to descipher their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"13  000  20  18  19  80  africanbaze  \\x89ûò  \\x89ûó  superintende  ancop  3  lanford  carolinaåêablaze  '@  diyala  r21  voortrekker  \\x89û  gtxrwm  2k13  30  2013  tinderbox  24  nashvilletraffic  8m  101  personalinjury  caraccidentlawyer  tee\\x89û  bigrigradio  77  31  6  1  sleepjunkies  08  06  15  11  03  58  40  ?'  ://  ld0uniyw4k  23  752  540  999  horndale  naayf  chandanee  conf\\x89û  293  3a  4  1600  17th  2015  2  09  2781  suffield  9  langtree  115  150  16  5  your4state  marinading  .@  norwaymfa  arrestpastornganga  .'  320  icemoon  ices\\x89û  xb1  wiedemer  \\x89ã¢  2010  full\\x89ã¢  ;&  wdyouth  ps4  8015  29  07  wednesday\\x89û  t48  mbataweel  freaky\\x89û  airplaneåê  ladins  \\x89ûïairplane\\x89û\\x9d  can\\x89ûªt  g90  12  emsne\\x89û  yugvani  14  ems1  17  \\x89û÷minimum  wage\\x89ûª  \\x89ûï  leoblakecarter  hatzolah  and\\x89û  freiza  petebests  dessicated  eovm  h\\x89û  70  100000  1960s  !'  zrnf  2005  fantasticfour  /#  fant4stic  \\x89û÷alloosh  quarterstaff  starmade  lanetary  ::  54000  withåêannihilation  gilbert23  tantonationalforest  bookslast  2011  fukurodani  bokuto  janenelson097  stephenscifi  05  02pm  2014  collegeradi\\x89û  m8  fittscott  0  !)  popularmmos  vi\\x89û  21  1023  candylit  sarumi  zonewolf123  mo\\x89û  4pm  coiffed  300  avysss  @'  russaky89  ((  ))  360  ophiuchus2613  1008pla\\x89û  //  hyider  ghost2  rtrrtcoach  1008planet\\x89û  fighterdena  20yrs  8  let\\x89ûªs  2a  ahamedis  125  dajaal  ?!  10  ww1  (#  cufi  preseasonworkouts  leejasper  interestraterise\\x89û\\x9d  !:  x1402  5sosfam  22  x1411  whitewalkers  lzktjnox  rally\\x89û  5th  100  ':  x1392  rea\\x89û  x1441  241487  .:  drayesha4  ki\\x89û  ìñ1  78  x1386  72  7  66  x1434  66obqmximb  \\x89û÷politics  grief\\x89ûª  relaxinpr  miprv  arson\\x89û  cbcto  tonight\\x89ûªs  nightbeat  :/  vigilent  nativehuman  myreligion  \\x89ûïhatchet  \\x89û\\x9d  udhampur  4head  \\x89ûª93  spos  handside  notley  contries  ignoranceshe  latinoand  benothing  morebut  2016  pugprobs  124  172  6615434  satoshis  robotcoingame  103  1998  99  171  1620  34  catechize  \\x89û÷avalanche\\x89ûª  4x4  chiasson  bigperm28  utahgrizz  kallemattson  b\\x89û  2nd  4kus  m27329  1236  !?!  du19  phantasmal  cummerbund  1916  800  ìñ  582  o784  4103  50  bioterror  aphl  åêfedex  28  htt\\x89û  alisonannyoung  orchardalley  foxnew\\x89û  bioter\\x89û  wxia  bioterrorism  90blks  8whts  ?@  infectiousdiseases  biolabs  biosurveillance  50ft  88  92  clergyforced  amesocialaction  bioterrorismi  tolewant  74  75  /@  rockefelleruniv  27  glanders  raisinfingers  547  flechadas  thisispublichealth  420  sothwest  fire\\x89û  qms8rressd  coast2coastdjs  jiwonle  1880  dmpl  +++++  m4  durvod  stoponesounds  107  32  hinatobot  s3xleak  ph0tos  19yrs  exp0sed  ([  ')  95  not\\x89û  elwoods  1921  ++++++  re\\x89û  100mb  q1  fpoj  landolina  2017  2019  hw18  90  fast2furious  wbc2015  1st  freyas  elxn42  stopharper  graywardens  magisters  lyrium  ;_&  prysmian  cobes  thda  greenspace  development\\x89û  iclown  purdies  apperception  bridgework  xxhjesc  37  locksmithing  lpdkl  5000  te\\x89û  ashayo  tweet4  overwatch  290  buroker  dialyses  -.-  712c  rolandonabeats  5sos  106  38  everwhe  blood\\x89û  butterlondon  ram0s  83  king\\x89ûªs  \\x89û÷the  tower\\x89ûª  thedarktower  okanowa  new\\x89û  a3  cleav  rs5  reafs  remorseless  s01e09  bloodymonday  s2g  pilloried  1862  we\\x89ûªre  we\\x89ûªve  bb17  tozlet  givebackkalinwhiteaccount  6th  dog\\x89ûªs  espn2  glononium  6c  nitroglycerin  å£279  00end  cultsierre  ameribag  å¤  en\\x89û  read\\x89û  35  419  longaberger  ìü  bosphore  bag\\x89û  _?  9973  '.  trollingtil  meekdiss  +:  dodxi41y1cåêis  mopheme  4got  ziphimup  2k15  ep18  womengirls  238  00  es\\x89û  98  \\x89û÷institute  peace\\x89ûª  moving2k15  expertwhiner  shop\\x89ûªs  cutekitten  summerinsweden  katterpì´instagram  dumle  dagens\\x89û  \\x89ûïparties  you\\x89ûªll  gasparc\\x89û  zicac  \\x89û÷body  bags\\x89ûª  norge2040  beforeitsnews  v\\x89û  dylanmcclure55  2008  whatcanthedo  ?:  ?!?!?  okinawan  1965  themale  200000  zergele  qendil  london3  shadowflame  emptive  raniakhalek  bannukes  vagersedolla  120000  3others  20000k  70th  schedule\\x89û  setting4  push2left  ='  '&  ;'  afghetc  sanitised  70\\x89û  generalnews  dambisa  siouxland  siouxlan  australia\\x89ûªs  60  90s\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words  in our data not in the embedding dictionary\n",
    "\"  \".join(not_covered[0:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unused words from the embedding dictionary are mainly symbols and foreign words. Which means we seem to have captured most of the important meaning-bearing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"  ！  。  、  …  \\'s  n\\'t  \\'m  ？  <  ・  ♥  “  ”  por  من  ♡  ´  ،  <heart>  aku  \\'re  una  （  >>  في  ･  le  é  ）  yg  —  笑  ω  je  yang  ❤  não  ～  ★  dia  و  الله  pero  ♪  \\'ll  nn  ｀  ¿  لا  ada  ☆  ni  \\'ve  على  ☺  ما  todo  mais  ini  ﾟ  aja  ▽  apa  cuando  ✔  ؟  •  quiero  kamu  nada  <<  ta  lagi  █  más  mau  ｡  pas  в  hoy  meu  gak  amor  \\\\  ver  ；  porque  اللهم  gue  uma  vou  bien  ¡  ＾  ／  todos  →  kan  kita  jajaja  você  nos  tengo  ∀  كل  bisa  iya  eso  gente  udah  ：  ＼  mejor  minha  ti  voy  tapi  nunca  il  nya  orang  °  اللي  tem  soy  este  kalo  só  \\'d  gracias  lah  и  ←  ⌣  estoy  tau  buat  я  kau  siempre  juga  casa  أن  feliz  bir  ahora  jadi  ka  vc  mundo  на  aqui  не  رتويت  يا  muy  mañana  mis  qué  mal  عن  día  dari  à  tiene  algo  ولا  c\\'est  hari  jajajaja  jaja  dah  ░  eres  está  nih  nak  hace  vamos  فولو  vez  dos  bueno  nao  dormir  quien  untuk  jangan  gua  д  wkwk  saya  mucho  ’s  baru  hacer  des  kok  j\\'  ان  cosas  xx  sih  deh  ela  bukan  hasta  akan  y\\'  isso  ✌  dong  lg  menos  dengan  ou  ˘  ele  ff  muito  hoje  ¬  dios  estar  já  quando  esa  ━  gw  banget  masih  qui  –  du  une  с  pues  ﾉ  mana  bom  siapa  suka  lebih  لي  alguien  ese  yaa  tidak  seu  إلا  ’  cada  定期  لك  een  kak  así  nem  عمل  esto  fue  yah  ه  ⭕  »  مع  agora  ☀  sigo  pq  dulu  foi  tudo  هو  あ  pagi  kalau  hati  sayang  baik  semua  hola  vos  tanto  tus  hora  estas  mah  quero  puede  sus  asi  ☹  ريتويت  tarde  sei  follback  selamat  ＿  sur  gusta  sabe  tp  tiempo  بس  sou  tuh  انا  das  anak  persona  essa  nadie  cinta  月  linda  het  ^_^  что  kk  لو  udh  n’t  sem  moi  aint  donde  bem  ng  punya  anda  tidur  puedo  personas  banyak  ✅  ➊  noche  tl  ＞  «  ku  pernah  pessoas  uno  unfollowers  mesmo  ⌒  puta  desde  makan  lindo  creo  sua  adalah  ang  mim  tenho  boleh  fazer  ＜  antes  بعد  toda  bgt  tener  kenapa  niet  boa  ●  dias  sobre  verdad  dar  dans  wat  هذا  يوم  amigos  todas  ∇  vas  а  semana  ➌  ➋  besok  jajajajaja  días  え  ter  ainda  ganas  çok  له  tú  var  veces  años  ♦  om  ça  buenos  igual  tienes  ‘  otra  sim  nanti  كان  ➍  عليه  falta  ♫  u.u  kali  mierda  tá  gitu  ─  sabes  ○  mu  sempre  cuenta  momento  nuevo  sudah  harus  sos  saber  ako  avec  غرد_بذكر_اللهn  قال  fin  الناس  ノ  ✋  по  pasti  quiere  cuma  ‾  mismo  emang  dalam  ▀  pake  kaya  لم  إذا  jgn  日  fotos  amigo  pasa  masa  acho  buenas  как  ▄  malam  entre  né  selalu  elle  شيء  mdr  buena  :o  año  lain  ؛  también  إلى  اذا  poco  lang  sakit  terus  إن  decir  claro  gk  amiga  для  vous  qe  pengen  buen  حتى  oke  è  ═  sono  deus  espero  متابعينك'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unused words in the embedding dictionary\n",
    "\"  \".join(unused_words[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.text[tweet.text.str.match(r\".*jonvoyage\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspellchecker\n",
    "# !conda install -c conda-forge pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "\n",
    "misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation copied from https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO implement into the data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "We need to convert the text to sequences as input to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus\n",
    "We first create a corpus, removing stop words\n",
    "- TODO: Try keeping stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(text: pd.core.series.Series):\n",
    "    \"\"\"\n",
    "    Creates a corpus while filtering out stop words and non-alphanumeric words\n",
    "    \"\"\"\n",
    "    \n",
    "    from nltk.tokenize import word_tokenize  # Tokenize words\n",
    "    \n",
    "    stop=set(stopwords.words('english'))\n",
    "    corpus = []\n",
    "    for tw in tqdm(text):   # For loop with progress bar\n",
    "        words = [word for word in word_tokenize(tw) if ((word.isalpha()) & (not word in stop))]\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a corpus from the training and test dataset. We will later split the data back into training and test data using the correct index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a corpus from the training and test dataset. We will later split the data back into training and test data using the correct index.\n",
    "corpus = create_corpus(tweet.text.append(test.text))\n",
    "save_obj(corpus, \"corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_obj(\"corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "We tokenize the tweets using helper functions from `keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer_object = Tokenizer()\n",
    "\n",
    "# Tokenize the words in the corpus\n",
    "tokenizer_object.fit_on_texts(corpus)\n",
    "\n",
    "# Create sequences from the text in the corpus. The words are now coded to numbers.\n",
    "sequences = tokenizer_object.texts_to_sequences(corpus)\n",
    "\n",
    "# We pad the tweets so that the model has input of equal length. Padding the start of text sequences seems to be the most widely used.\n",
    "MAX_LEN = 50\n",
    "tweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='pre', padding='pre')\n",
    "\n",
    "# Save the tokenizer object\n",
    "save_obj(tokenizer_object, \"tokenizer_object\")\n",
    "save_obj(tweet_pad, \"tweet_pad\")\n",
    "\n",
    "# Load previously created tokenizer\n",
    "tokenizer_object = load_obj(\"tokenizer_object\")\n",
    "tweet_pad = load_obj(\"tweet_pad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embedding matrix\n",
    "We create the embedding matrix, where each row is a pre-trained word embedding vector from the GloVe dictionary, and where row index coincides with the word token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO:For the index in the matrix to match the word tokens the embedding_matrix is padded with an initial row of zeros because tokenization starts at 1, while python list index starts at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer_object.word_index\n",
    "num_words = len(word_index) + 1\n",
    "\n",
    "# embedding_matrix = np.zeros((num_words, 100))\n",
    "\n",
    "# for word,i in tqdm(word_index.items(), total = len(word_index.items())):\n",
    "#     emb_vec=embedding_dict.get(word)\n",
    "#     if emb_vec is not None:\n",
    "#         embedding_matrix[i,:]=emb_vec\n",
    "\n",
    "# save_obj(embedding_matrix, \"embedding_matrix\")\n",
    "\n",
    "embedding_matrix = load_obj(\"embedding_matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize embedding layer\n",
    "The embedding layer is a flexible layer that can be used in a variety of ways:\n",
    "- It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
    "- It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
    "- It can be used to load a pre-trained word embedding model, a type of transfer learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "embedding = Embedding(input_dim  = num_words,\n",
    "                      output_dim = 100,\n",
    "                      embeddings_initializer = Constant(embedding_matrix),\n",
    "                      input_length = MAX_LEN,\n",
    "                      mask_zero = True,  # Boolean, whether or not the input value 0 is a special \"padding\" value that should be masked out.\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation\n",
    "We now create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "We then set up our model\n",
    "- TODO: What type of dropout to apply? (see discussion here https://github.com/keras-team/keras/issues/7290)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.8))  # Dropout to reduce overfitting.\n",
    "model.add(LSTM(64,\n",
    "               dropout=0.25,  # Fraction of the units to drop for the linear transformation of the inputs.\n",
    "               recurrent_dropout=0.25  # Fraction of the units to drop for the linear transformation of the recurrent state.\n",
    "              ))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test set\n",
    "We split the training data into a training and test set for model optimization.\n",
    "We choose test set size to be 15 % of the data, the remaining 85 % is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#We split our data sequences back into the original training and test data.\n",
    "train_data = tweet_pad[:7613, :]\n",
    "test_data = tweet_pad[7613:, :]\n",
    "\n",
    "# We split the training data into a training and test set for the model fitting\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_data,\n",
    "                                                    tweet['target'].values,\n",
    "                                                    test_size=0.15)\n",
    "print('Shape of train',x_train.shape)\n",
    "print(\"Shape of Validation \",x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=16,\n",
    "                    epochs=9,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=2)\n",
    "\n",
    "# Save the fitted model to a file\n",
    "model.save('./obj/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to overfit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the fitted model from file:\n",
    "\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model('./obj/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = sns.lineplot(data=pd.DataFrame(history.history)[[\"accuracy\", \"loss\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = sns.lineplot(data=pd.DataFrame(history.history)[[\"val_accuracy\", \"val_loss\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and loss seem to decrease on the validation set. This does not bode well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- Inspect misclassified tweets\n",
    "- Compare to pretrained CNN\n",
    "- Try larger word embedding\n",
    "- Experiment with dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misclassified tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Remove non words\n",
    "- encode urls?\n",
    "- Encode complexity of text: https://pypi.org/project/textstat/\n",
    "- Removal of stop words (library for specific language)\n",
    "- Remove URL's\n",
    "- Spelling/grammar correction?\n",
    "  - Companies like Google and Microsoft have achieved a decent accuracy level in automated spell correction. One can use algorithms like the Levenshtein Distances, Dictionary Lookup etc. or other modules and packages to fix these errors.\n",
    "  - Number of misspelled words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try Neural Turing Machine (attention): https://github.com/carpedm20/NTM-tensorflow\n",
    "- End-To-End Memory Networks: https://github.com/carpedm20/MemN2N-tensorflow\n",
    "- Adaptive Computation Time algorithm https://github.com/DeNeutoy/act-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split joined words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hei Og Haa'"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(re.findall('[A-Z][^A-Z]*', \"HeiOgHaa\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate letters\n",
    "Needs improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I loove you!'"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "''.join(''.join(s)[:2] for _, s in itertools.groupby(\"I loooove you!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords\n",
    "We can use NLTK to remove common words.\n",
    "These words contain little information on their own, but they might convey information in the sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/jonaso/nltk_data'\n    - '/home/jonaso/anaconda3/nltk_data'\n    - '/home/jonaso/anaconda3/share/nltk_data'\n    - '/home/jonaso/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/jonaso/nltk_data'\n    - '/home/jonaso/anaconda3/nltk_data'\n    - '/home/jonaso/anaconda3/share/nltk_data'\n    - '/home/jonaso/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3181/3054741396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/jonaso/nltk_data'\n    - '/home/jonaso/anaconda3/nltk_data'\n    - '/home/jonaso/anaconda3/share/nltk_data'\n    - '/home/jonaso/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
