{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-disaster-tweets-using-torch-RoBERTa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNEM59JqVrAoJoEjrwF59h0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Orenjonas/natural_language_processing_with_disaster_tweets/blob/main/NLP_disaster_tweets_using_torch_RoBERTa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmMCyeL-6aB6"
      },
      "source": [
        "# Introduction\n",
        "Using a RoBERTa model pretrained on twitter data for classifying tweets as relating to real disasters or not.\n",
        "\n",
        "Model description can be found [here](https://huggingface.co/cardiffnlp/twitter-roberta-base)\n",
        "\n",
        "TODO:\n",
        "- More detailed text cleaning.\n",
        "- Inspect mislassified [example](https://colab.research.google.com/github/markwest1972/LSTM-Example-Google-Colaboratory/blob/master/LSTM_IMDB_Sentiment_Example.ipynb#scrollTo=rpCS2-jFH1KY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na3uS6rL4WEs"
      },
      "source": [
        "# Import data\n",
        "## Mount google drive to notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHapGBU9zzR8",
        "outputId": "d331d178-479e-49a2-be1c-61d87afb1834"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AX4XfWj99TiuoESoe2ipQIK7MffF9sAF5Gdd3FUQtCn3Wkap4gjJXydb_0E\n",
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqExrK_452Wa",
        "outputId": "b3c47eb4-b424-42fb-f0c3-7740c02f340c"
      },
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/gdrive/My Drive/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/input/glove.twitter.27B.100d.txt\n",
            "/gdrive/My Drive/input/glove.twitter.27B.25d.txt\n",
            "/gdrive/My Drive/input/glove.twitter.27B.50d.txt\n",
            "/gdrive/My Drive/input/glove.twitter.27B.zip\n",
            "/gdrive/My Drive/input/nlp-getting-started/test.csv\n",
            "/gdrive/My Drive/input/nlp-getting-started/sample_submission.csv\n",
            "/gdrive/My Drive/input/nlp-getting-started/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g284Y0pFDzUr",
        "outputId": "3cd0c428-c84d-44ed-ac2f-faab30a72403"
      },
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "# raw_dataset = load_dataset('csv', data_files='/gdrive/My Drive/input/nlp-getting-started/train.csv')\n",
        "# raw_competition_test_dataset = load_dataset('csv', data_files='/gdrive/My Drive/input/nlp-getting-started/test.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.12.1-py3-none-any.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 54.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 46.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0,>=0.0.14\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 43.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.14->datasets) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 52.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 66.8 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.12.1 fsspec-2021.8.1 huggingface-hub-0.0.17 multidict-5.1.0 xxhash-2.0.2 yarl-1.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWPd1WLqkWnE"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "raw_dataset = pd.read_csv('/gdrive/My Drive/input/nlp-getting-started/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_sih_NZkhud"
      },
      "source": [
        "raw_dataset.drop_duplicates(subset=['text'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7XpybH_tozZ",
        "outputId": "c154067c-41b2-4725-e491-c11c39f7820b"
      },
      "source": [
        "!sudo apt-get install enchant\n",
        "!pip install pyenchant"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
            "  | openoffice.org-core libenchant-voikko\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
            "  libaspell15 libenchant1c2a libhunspell-1.6-0 libtext-iconv-perl\n",
            "0 upgraded, 10 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 1,312 kB of archives.\n",
            "After this operation, 5,353 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtext-iconv-perl amd64 1.7-5build6 [13.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaspell15 amd64 0.60.7~20110707-4ubuntu0.2 [310 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 emacsen-common all 2.0.8 [17.6 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 dictionaries-common all 1.27.2 [186 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 aspell amd64 0.60.7~20110707-4ubuntu0.2 [87.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 aspell-en all 2017.08.24-0-0.1 [298 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 hunspell-en-us all 1:2017.08.24 [168 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhunspell-1.6-0 amd64 1.6.2-1 [154 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libenchant1c2a amd64 1.6.0-11.1 [64.4 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 enchant amd64 1.6.0-11.1 [12.2 kB]\n",
            "Fetched 1,312 kB in 1s (1,076 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 10.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "(Reading database ... 155013 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libtext-iconv-perl_1.7-5build6_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-5build6) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../1-libaspell15_0.60.7~20110707-4ubuntu0.2_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Selecting previously unselected package emacsen-common.\n",
            "Preparing to unpack .../2-emacsen-common_2.0.8_all.deb ...\n",
            "Unpacking emacsen-common (2.0.8) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../3-dictionaries-common_1.27.2_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.27.2) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../4-aspell_0.60.7~20110707-4ubuntu0.2_amd64.deb ...\n",
            "Unpacking aspell (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../5-aspell-en_2017.08.24-0-0.1_all.deb ...\n",
            "Unpacking aspell-en (2017.08.24-0-0.1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../6-hunspell-en-us_1%3a2017.08.24_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2017.08.24) ...\n",
            "Selecting previously unselected package libhunspell-1.6-0:amd64.\n",
            "Preparing to unpack .../7-libhunspell-1.6-0_1.6.2-1_amd64.deb ...\n",
            "Unpacking libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Selecting previously unselected package libenchant1c2a:amd64.\n",
            "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Selecting previously unselected package enchant.\n",
            "Preparing to unpack .../9-enchant_1.6.0-11.1_amd64.deb ...\n",
            "Unpacking enchant (1.6.0-11.1) ...\n",
            "Setting up libhunspell-1.6-0:amd64 (1.6.2-1) ...\n",
            "Setting up libaspell15:amd64 (0.60.7~20110707-4ubuntu0.2) ...\n",
            "Setting up emacsen-common (2.0.8) ...\n",
            "Setting up libtext-iconv-perl (1.7-5build6) ...\n",
            "Setting up dictionaries-common (1.27.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up aspell (0.60.7~20110707-4ubuntu0.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up hunspell-en-us (1:2017.08.24) ...\n",
            "Setting up libenchant1c2a:amd64 (1.6.0-11.1) ...\n",
            "Setting up aspell-en (2017.08.24-0-0.1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up enchant (1.6.0-11.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for dictionaries-common (1.27.2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "Collecting pyenchant\n",
            "  Downloading pyenchant-3.2.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyenchant\n",
            "Successfully installed pyenchant-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0no1Vwk-z3aQ"
      },
      "source": [
        "# Clean data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS2GQ82luBx7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsxE2VH-k8Ce"
      },
      "source": [
        "\n",
        "def clean_tweets(df):\n",
        "    from itertools import combinations\n",
        "    import re\n",
        "\n",
        "    def sub(pattern, output, string, whole_word=False):\n",
        "        token = output\n",
        "        if whole_word:\n",
        "            pattern = r'(\\s|^)' + pattern + r'(\\s|$)'\n",
        "\n",
        "        if isinstance(output, str):\n",
        "            token = ' ' + output + ' '\n",
        "        else:\n",
        "            token = lambda match: ' ' + output(match) + ' '\n",
        "\n",
        "        return re.sub(pattern, token, string)\n",
        "\n",
        "\n",
        "    def hashtag(token):\n",
        "        \"\"\" Replace hashtag `#` with `<hashtag>` and split following joined words.\"\"\"\n",
        "        token = token.group('tag')\n",
        "        if token != token.upper():\n",
        "            token = ' '.join(re.findall('[a-zA-Z][^A-Z]*', token))\n",
        "\n",
        "        return '<hashtag> ' + token\n",
        "\n",
        "    def punc_repeat(token):\n",
        "        return token.group(0)[0] + \" <repeat>\"\n",
        "\n",
        "    def punc_separate(token):\n",
        "        return token.group()\n",
        "\n",
        "    def number(token):\n",
        "        return token.group() + ' <number>';\n",
        "\n",
        "    def word_end_repeat(token):\n",
        "        return token.group(1) + token.group(2) + ' <elong>'\n",
        "    \n",
        "    def allcaps(token):\n",
        "        return token.group() + ' <allcaps>'\n",
        "\n",
        "    def clean_repeated_letters(tweet: str):\n",
        "        \"\"\"\n",
        "        Splits a tweet into words, finds repeated letters in the word and\n",
        "        removes combinations of the repeated letters until the word is matched by a key in\n",
        "        the english dicitonary\n",
        "        \"\"\"\n",
        "\n",
        "        # English dictionary\n",
        "        import enchant\n",
        "        d = enchant.Dict(\"en_US\")\n",
        "\n",
        "        cleaned_tweet = []\n",
        "\n",
        "        for word_i in tweet.split():\n",
        "            word_found = False\n",
        "\n",
        "            # Check that word is in the english library\n",
        "            if d.check(word_i):\n",
        "                cleaned_tweet.append(word_i)\n",
        "                continue\n",
        "\n",
        "            matches = re.findall(r\"\"\"(\\S*?)    # 1: Optional preceeding letters\n",
        "                                     (\\w)      # 2: A letter that might be repeated\n",
        "                                     (\\2{1,})  # 3: Repetead instances of the preceeding letter (group 2)\n",
        "                                     (\\S*?)    # 4: Optional trailing letters\"\"\",\n",
        "                                 word_i,\n",
        "                                 flags=re.X)  # Verbose regex, for commenting\n",
        "                                 \n",
        "            repeated_letters = [match[2] for match in matches]\n",
        "                    \n",
        "            # Loop over all combinations of repeated letters\n",
        "            for i in range(len(repeated_letters), 0, -1):  # i decides length of combination\n",
        "                if word_found:\n",
        "                    continue\n",
        "                    \n",
        "                for combination in combinations(repeated_letters, r = i):\n",
        "                    if word_found:\n",
        "                        continue\n",
        "                        \n",
        "                    tword = word_i \n",
        "                    \n",
        "                        \n",
        "                    for letters in combination:\n",
        "                        tword = re.sub(letters, \"\", tword, count=1)\n",
        "                                        \n",
        "                        # Word in the english dictionary?\n",
        "                        if d.check(tword):\n",
        "                            # Keep the word and stop searching\n",
        "                            word_found = True\n",
        "                            tword = tword + \" <elong>\"\n",
        "                            continue  \n",
        "            if not word_found:\n",
        "                # No match, we simply keep the word\n",
        "                tword = word_i\n",
        "                \n",
        "            cleaned_tweet.append(tword)\n",
        "            \n",
        "        return \" \".join(cleaned_tweet)\n",
        "\n",
        "\n",
        "\n",
        "    eyes        = r\"[8:=;]\"\n",
        "    nose        = r\"['`\\-\\^]?\"\n",
        "    sad_front   = r\"[(\\[/\\\\]+\"\n",
        "    sad_back    = r\"[)\\]/\\\\]+\"\n",
        "    smile_front = r\"[)\\]]+\"\n",
        "    smile_back  = r\"[(\\[]+\"\n",
        "    lol_front   = r\"[DbpP]+\"\n",
        "    lol_back    = r\"[d]+\"\n",
        "    neutral     = r\"[|]+\"\n",
        "    sadface     = eyes + nose + sad_front   + '|' + sad_back   + nose + eyes\n",
        "    smile       = eyes + nose + smile_front + '|' + smile_back + nose + eyes\n",
        "    lolface     = eyes + nose + lol_front   + '|' + lol_back   + nose + eyes\n",
        "    neutralface = eyes + nose + neutral     + '|' + neutral    + nose + eyes\n",
        "    punctuation = r\"\"\"[ '!\"#$%&'()+,/:;=?@_`{|}~\\*\\-\\.\\^\\\\\\[\\]]+\"\"\" ## < and > omitted to avoid messing up tokens\n",
        "\n",
        "    # Remove contractions\n",
        "    contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                        \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
        "                        \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                        \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                        \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                        \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                        \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                        \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                        \"i'd\": \"i would\", \"i'd've\": \"i would have\",\"i'll\": \"i will\",\n",
        "                        \"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\", \"isn't\": \"is not\",\n",
        "                        \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                        \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                        \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                        \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                        \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                        \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                        \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                        \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                        \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                        \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                        \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                        \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                        \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                        \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                        \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                        \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                        \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                        \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                        \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                        \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                        \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                        \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                        \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                        \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                        \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                        \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                        \"you've\": \"you have\"}\n",
        "    \n",
        "    # Regular expression for finding contractions\n",
        "    #    adding positive lookbehind for \"'s\" in the regex to make sure a letter is preceeding\n",
        "    contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()).lower().replace(\"|'s\", \"|(?<=[a-zA-Z])'s\"))\n",
        "\n",
        "    def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "        def replace(match):\n",
        "            return contractions_dict[match.group(1)]\n",
        "        return contractions_re.sub(replace, text)\n",
        "\n",
        "\n",
        "    for i in df.index:\n",
        "        df.loc[i,'text'] = sub(r'[\\s]+',                             '  ',            df.loc[i,'text']) # ensure 2 spaces between everything\n",
        "        df.loc[i,'text'] = sub(r'(?:(?:https?|ftp)://|www\\.)[^\\s]+', 'http',          df.loc[i,'text'], True)\n",
        "        df.loc[i,'text'] = sub(r'@\\w+',                              '@user',         df.loc[i,'text'], True)\n",
        "        df.loc[i,'text'] = sub(r'#(?P<tag>\\w+)',                     hashtag,         df.loc[i,'text'], True)\n",
        "        df.loc[i,'text'] = sub(sadface,                              '<sadface>',     df.loc[i,'text'], True)\n",
        "        df.loc[i,'text'] = sub(smile,                                '<smile>',       df.loc[i,'text'], True)\n",
        "        df.loc[i,'text'] = sub(lolface,                              '<lolface>',     df.loc[i,'text'], True)\n",
        "        df.loc[i,'text'] = sub(neutralface,                          '<neutralface>', df.loc[i,'text'], True)\n",
        "        df.loc[i,'text'] = sub(r'(?:<3+)+',                          '<heart>',       df.loc[i,'text'], True)\n",
        "        df.loc[i,'text'] = sub(r'\\b[A-Z]+\\b',                        allcaps,         df.loc[i,'text'], True) \n",
        "        df.loc[i,'text'] =                                           df.loc[i,'text'].lower()\n",
        "        df.loc[i,'text'] =                                           expand_contractions(df.loc[i, 'text'])\n",
        "        df.loc[i,'text'] = sub(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*',          number,          df.loc[i,'text'], True)\n",
        "        df.loc[i,'text'] = sub(punctuation,                          punc_separate,   df.loc[i,'text'])\n",
        "        df.loc[i,'text'] = sub(r'([!?.])\\1+',                        punc_repeat,     df.loc[i,'text'])\n",
        "        \n",
        "        df.loc[i,'text'] = clean_repeated_letters(df.loc[i,'text'])\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RuVlRaw0nZDs"
      },
      "source": [
        "cleaned_data = clean_tweets(df = raw_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PMzKfvIZcpHJ",
        "outputId": "63e3af11-0310-4504-bf1f-43da67b2fee1"
      },
      "source": [
        "raw_dataset.iloc[48]['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'first night with retainers in . it is quite weird . better get used to it ; i <allcaps> have to wear them every single night for the next year at least .'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LuUOErACccpq",
        "outputId": "1a6a9973-2961-4838-895c-5188649fc7bb"
      },
      "source": [
        "raw_dataset.index[48]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FpfPIb0XcDeA",
        "outputId": "178bce88-cc9a-4258-9b12-ee9bd46c95fa"
      },
      "source": [
        "raw_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>our deeds are the reason of this &lt;hashtag&gt; ear...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>forest fire near la ronge sask . canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>all residents asked to ' shelter in place ' ar...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13 , 000 &lt;number&gt; people receive &lt;hashtag&gt; wil...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>just got sent this photo from ruby &lt;hashtag&gt; a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7604</th>\n",
              "      <td>10863</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>&lt;hashtag&gt; world news fallen powerlines on g : ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7605</th>\n",
              "      <td>10864</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>on the flip side i am at walmart and there is ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7606</th>\n",
              "      <td>10866</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>suicide bomber kills 15 &lt;number&gt; in saudi secu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7608</th>\n",
              "      <td>10869</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>two giant cranes holding a bridge collapse int...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7612</th>\n",
              "      <td>10873</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>the latest : more homes razed by northern cali...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7503 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id keyword  ...                                               text target\n",
              "0         1     NaN  ...  our deeds are the reason of this <hashtag> ear...      1\n",
              "1         4     NaN  ...            forest fire near la ronge sask . canada      1\n",
              "2         5     NaN  ...  all residents asked to ' shelter in place ' ar...      1\n",
              "3         6     NaN  ...  13 , 000 <number> people receive <hashtag> wil...      1\n",
              "4         7     NaN  ...  just got sent this photo from ruby <hashtag> a...      1\n",
              "...     ...     ...  ...                                                ...    ...\n",
              "7604  10863     NaN  ...  <hashtag> world news fallen powerlines on g : ...      1\n",
              "7605  10864     NaN  ...  on the flip side i am at walmart and there is ...      1\n",
              "7606  10866     NaN  ...  suicide bomber kills 15 <number> in saudi secu...      1\n",
              "7608  10869     NaN  ...  two giant cranes holding a bridge collapse int...      1\n",
              "7612  10873     NaN  ...  the latest : more homes razed by northern cali...      1\n",
              "\n",
              "[7503 rows x 5 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rHk16_Vk0x5n"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "from datasets import Dataset\n",
        "\n",
        "cleaned_data = Dataset.from_pandas(cleaned_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l6sxam2j3nB8",
        "outputId": "4be8e836-5791-4a77-8e6b-95a6cfecbcc1"
      },
      "source": [
        "# Inspect cleaned tweet text\n",
        "\n",
        "cleaned_data['text'][2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"all residents asked to ' shelter in place ' are being notified by officers . no other evacuation or shelter in place orders are expected\""
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L50k4aZ45T2o"
      },
      "source": [
        "# Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "MWLH8UoI5VYG"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "355a432ba5e34eef994e1299c63a6086"
          ]
        },
        "collapsed": true,
        "id": "nSijJgTQGUU3",
        "outputId": "1d0fb0fb-5f96-4f7e-ffd3-bf8623b4f2d0"
      },
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "tokenized_data = cleaned_data.map(tokenize_function, batched=True)\n",
        "\n",
        "# # TODO: adapt code\n",
        "# MAX_LEN = \n",
        "# tokenized_train_data = processed_train_data.map(tokenize_function, batched=True)\n",
        "# tokenized_test_data = processed_test_data.map(tokenize_function, batched=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "355a432ba5e34eef994e1299c63a6086",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/8 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8GOVmpvzPCS0"
      },
      "source": [
        "# Split training data into a dictionary containing a test and training (validation) set\n",
        "\n",
        "tokenized_datasets = tokenized_data.train_test_split(test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kBVAGq3_MV-D"
      },
      "source": [
        "small_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset =  tokenized_datasets['test'].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "# full_train_dataset =  tokenized_datasets[\"train\"]\n",
        "# full_eval_dataset =   tokenized_datasets[\"test\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLYoWE4TV3tF"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_-2udkMVXiWy"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_tf_dataset = small_train_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
        "eval_tf_dataset = small_eval_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2S-1I0zy5TkN"
      },
      "source": [
        "train_tf_dataset = small_train_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
        "eval_tf_dataset  = small_eval_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BE3RNglz5wzL",
        "outputId": "5e36e7fe-2aff-4419-ce78-7dfce712d3e8"
      },
      "source": [
        "\n",
        "def sliceDataset_to_batchDataset(dataset):\n",
        "    features = {x: dataset[x].to_tensor() for x in tokenizer.model_input_names}\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features, dataset[\"target\"]))\n",
        "    return dataset.shuffle(len(dataset)).batch(8)\n",
        "\n",
        "\n",
        "train_tf_dataset = sliceDataset_to_batchDataset(train_tf_dataset)\n",
        "eval_tf_dataset = sliceDataset_to_batchDataset(eval_tf_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/datasets/formatting/formatting.py:167: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(array, copy=False, **self.np_array_kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "53059d700f884945a84f02c14bc12bec"
          ]
        },
        "id": "UKuXsIxUV4u3",
        "outputId": "14ea297b-7a40-4e9d-f4a4-63e4b29152b6"
      },
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "# Initiate a tensorflow model from the pretrained model. Will throw a warning about some layers not bein initialized\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53059d700f884945a84f02c14bc12bec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y1cthVJTZS9X",
        "outputId": "8cdd17af-576e-47bc-cbec-ba7982ded6bc"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=4e-6),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_roberta_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "roberta (TFRobertaMainLayer) multiple                  124055040 \n",
            "_________________________________________________________________\n",
            "classifier (TFRobertaClassif multiple                  592130    \n",
            "=================================================================\n",
            "Total params: 124,647,170\n",
            "Trainable params: 124,647,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_ZjrVjM5hAM6",
        "outputId": "c95f9683-de8b-41fb-b18d-6272f7b66390"
      },
      "source": [
        "\n",
        "history = model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "125/125 [==============================] - 1852s 15s/step - loss: 0.6807 - sparse_categorical_accuracy: 0.5350 - val_loss: 0.6273 - val_sparse_categorical_accuracy: 0.5740\n",
            "Epoch 2/5\n",
            "125/125 [==============================] - 1819s 15s/step - loss: 0.4827 - sparse_categorical_accuracy: 0.7920 - val_loss: 0.4237 - val_sparse_categorical_accuracy: 0.8300\n",
            "Epoch 3/5\n",
            "125/125 [==============================] - 1800s 14s/step - loss: 0.3722 - sparse_categorical_accuracy: 0.8470 - val_loss: 0.4376 - val_sparse_categorical_accuracy: 0.8280\n",
            "Epoch 4/5\n",
            "125/125 [==============================] - 1664s 13s/step - loss: 0.2878 - sparse_categorical_accuracy: 0.8930 - val_loss: 0.4409 - val_sparse_categorical_accuracy: 0.8150\n",
            "Epoch 5/5\n",
            "125/125 [==============================] - 1840s 15s/step - loss: 0.2287 - sparse_categorical_accuracy: 0.9280 - val_loss: 0.4994 - val_sparse_categorical_accuracy: 0.8150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b-NUQgX7Zfx"
      },
      "source": [
        "# Save the model\n",
        "!mkdir -p saved_model\n",
        "model.save('saved_model/tweet_roberta_trained')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPfuTFb47xYS"
      },
      "source": [
        "# # Load model\n",
        "# model = tf.keras.models.load_model('saved_model/tweet_roberta_trained')\n",
        "\n",
        "# # Check its architecture\n",
        "# new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaLUs_bq89Is"
      },
      "source": [
        "## Something to try:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib5L1cjY8yAF"
      },
      "source": [
        "# # Add dense layer after transformer model (RoBERTa)\n",
        "\n",
        "\n",
        "# # Import the needed model(Bert, Roberta or DistilBert) with output_hidden_states=True\n",
        "# transformer_model = TFBertForSequenceClassification.from_pretrained('bert-large-cased', output_hidden_states=True)\n",
        "\n",
        "# input_ids = tf.keras.Input(shape=(128, ),dtype='int32')\n",
        "# attention_mask = tf.keras.Input(shape=(128, ), dtype='int32')\n",
        "\n",
        "# transformer = transformer_model([input_ids, attention_mask])    \n",
        "# hidden_states = transformer[1] # get output_hidden_states\n",
        "\n",
        "# hidden_states_size = 4 # count of the last states \n",
        "# hiddes_states_ind = list(range(-hidden_states_size, 0, 1))\n",
        "\n",
        "# selected_hiddes_states = tf.keras.layers.concatenate(tuple([hidden_states[i] for i in hiddes_states_ind]))\n",
        "\n",
        "# # Now we can use selected_hiddes_states as we want\n",
        "# output = tf.keras.layers.Dense(128, activation='relu')(selected_hiddes_states)\n",
        "# output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\n",
        "# model = tf.keras.models.Model(inputs = [input_ids, attention_mask], outputs = output)\n",
        "# model.compile(tf.keras.optimizers.Adam(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}