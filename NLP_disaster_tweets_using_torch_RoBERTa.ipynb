{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-disaster-tweets-using-torch-RoBERTa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNBn3zura/6irmCLV/X8RYB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Orenjonas/natural_language_processing_with_disaster_tweets/blob/main/NLP_disaster_tweets_using_torch_RoBERTa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmMCyeL-6aB6"
      },
      "source": [
        "# Introduction\n",
        "Using a RoBERTa model pretrained on twitter data for classifying tweets as relating to real disasters or not.\n",
        "\n",
        "Model description can be found [here](https://huggingface.co/cardiffnlp/twitter-roberta-base)\n",
        "\n",
        "TODO:\n",
        "- More detailed text cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na3uS6rL4WEs"
      },
      "source": [
        "# Import data\n",
        "## Mount google drive to notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHapGBU9zzR8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axbyEYio4Mgn"
      },
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqExrK_452Wa"
      },
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/gdrive/My Drive/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g284Y0pFDzUr"
      },
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_dataset = load_dataset('csv', data_files='/gdrive/My Drive/input/nlp-getting-started/train.csv')\n",
        "raw_competition_test_dataset = load_dataset('csv', data_files='/gdrive/My Drive/input/nlp-getting-started/test.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb_QQuBHD6T7"
      },
      "source": [
        "raw_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evfd-74q6Wkl"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiHOn8Ji6Te5"
      },
      "source": [
        "def preprocess(row):\n",
        "    text = row['text']\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    row['text'] = \" \".join(new_text)\n",
        "    return row\n",
        "    # return \" \".join(new_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApAkjx2c81yG"
      },
      "source": [
        "# Data dict currently only concist of a 'train' entry.\n",
        "#  We will later split this into a training and evaluation set\n",
        "processed_data = raw_dataset['train'].map(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7XxnEpkFGEU"
      },
      "source": [
        "## TODO: Further data cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L50k4aZ45T2o"
      },
      "source": [
        "# Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MWLH8UoI5VYG"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "MODEL = \"cardiffnlp/twitter-roberta-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSijJgTQGUU3",
        "collapsed": true
      },
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "tokenized_data = processed_data.map(tokenize_function, batched=True)\n",
        "\n",
        "# # TODO: adapt code\n",
        "# MAX_LEN = \n",
        "# tokenized_train_data = processed_train_data.map(tokenize_function, batched=True)\n",
        "# tokenized_test_data = processed_test_data.map(tokenize_function, batched=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GOVmpvzPCS0"
      },
      "source": [
        "# Split training data into a dictionary containing a test and training (validation) set\n",
        "tokenized_datasets = tokenized_data.train_test_split(test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBVAGq3_MV-D"
      },
      "source": [
        "small_train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset =  tokenized_datasets['test'].shuffle(seed=42).select(range(1000))\n",
        "\n",
        "full_train_dataset =  tokenized_datasets[\"train\"]\n",
        "full_eval_dataset =   tokenized_datasets[\"test\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJXuQrX284XU"
      },
      "source": [
        "\n",
        "encoded_input = tokenizer(text, return_tensors='tf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLYoWE4TV3tF"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-2udkMVXiWy"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_tf_dataset = small_train_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
        "eval_tf_dataset  = small_eval_dataset.remove_columns([\"text\"]).with_format(\"tensorflow\")\n",
        "\n",
        "def sliceDataset_to_batchDataset(dataset):\n",
        "    features = {x: dataset[x].to_tensor() for x in tokenizer.model_input_names}\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((train_features, dataset[\"target\"]))\n",
        "    return dataset.shuffle(len(dataset)).batch(8)\n",
        "\n",
        "\n",
        "train_tf_dataset = sliceDataset_to_batchDataset(train_tf_dataset)\n",
        "eval_tf_dataset = sliceDataset_to_batchDataset(eval_tf_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKuXsIxUV4u3"
      },
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "# Initiate a tensorflow model from the pretrained model. Will throw a warning about some layers not bein initialized\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJRewMUJaSlx"
      },
      "source": [
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1cthVJTZS9X"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZjrVjM5hAM6"
      },
      "source": [
        "\n",
        "history = model.fit(train_tf_dataset, validation_data=eval_tf_dataset, epochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOWSnrmZmhpm"
      },
      "source": [
        "## Model is overfitting\n",
        "- Try more detailed cleaning.\n",
        "- Try another pretrained model, e.g roberta-base."
      ]
    }
  ]
}