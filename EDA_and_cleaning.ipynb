{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook implements cleaning and data exploration of the twitter data for the \n",
    "[Kaggle competition](https://www.kaggle.com/c/nlp-getting-started) on predicting which tweets refer to actual disasters.\n",
    "\n",
    "It is an exploratory work in progress, containing some unfinished tasks and some thoughts.\n",
    "\n",
    "## Layout\n",
    "The notebook covers\n",
    "- Basic inspection of the data\n",
    "- Sentiment analysis using [nltk](https://www.nltk.org/).\n",
    "- Implementing word vectorization using the GloVe embeddings pre-trained on twitter data.\n",
    "- Text cleaning\n",
    "  - A general text cleanin method for analysis\n",
    "  - Text cleaning specific for the GloVe embeddings\n",
    " \n",
    "## Thoughts\n",
    "- Separate model for each keyword in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Data processing\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Other\n",
    "from tqdm import tqdm  # Progress bar\n",
    "from IPython.display import display, Markdown  # For printing markdown formatted output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Saving objects\n",
    "We create some reuseable code for saving objects for later use, so we don't have to re-run time consuming code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet = pd.read_csv('./input/nlp-getting-started/train.csv')\n",
    "test  = pd.read_csv('./input/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n",
      "(3263, 4)\n"
     ]
    }
   ],
   "source": [
    "print(tweet.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data contains 7613 observations, while the test data contains 3263 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>281</td>\n",
       "      <td>ambulance</td>\n",
       "      <td>VISIT MY YOUTUBE CHANNEL.</td>\n",
       "      <td>HAPPENING NOW - HATZOLAH EMS AMBULANCE RESPOND...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>283</td>\n",
       "      <td>ambulance</td>\n",
       "      <td>Lexington</td>\n",
       "      <td>http://t.co/FueRk0gWui Twelve feared killed in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>285</td>\n",
       "      <td>ambulance</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://t.co/X5YEUYLT1X Twelve feared killed in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    keyword                   location  \\\n",
       "200  281  ambulance  VISIT MY YOUTUBE CHANNEL.   \n",
       "201  283  ambulance                  Lexington   \n",
       "202  285  ambulance                        NaN   \n",
       "\n",
       "                                                  text  target  \n",
       "200  HAPPENING NOW - HATZOLAH EMS AMBULANCE RESPOND...       0  \n",
       "201  http://t.co/FueRk0gWui Twelve feared killed in...       1  \n",
       "202  http://t.co/X5YEUYLT1X Twelve feared killed in...       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.iloc[200:203]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data rows contain a `keyword`, a `location` (sometimes not present), `text` containg a tweet and a `target` coding 1 for disaster and 0 for non-disaster.\n",
    "\n",
    "At first glance location does not seem trustworthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting a few tweets we see that we need to clean the text before we can analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Target: 0 -- https://t.co/WKv8VqVkT6 #ArtisteOfTheWeekFact say #Conversations by #coast2coastdjs agree @Crystal_Blaz 's #Jiwonle is a #HipHop #ClubBanger"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 1 -- RSS: Russia begins mass destruction of illegally imported food   http://t.co/r6JDj9kIGm"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 1 -- Emergency Flow  http://t.co/lH9mrYpDrJ mp3 http://t.co/PqhuthSS3i rar http://t.co/0iW6dRf5X9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 1 -- 8/6/2015@2:09 PM: TRAFFIC ACCIDENT NO INJURY at 2781 WILLIS FOREMAN RD http://t.co/VCkIT6EDEv"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 0 -- Absurdly Ridiculous MenÛªs #Fashion To Demolish You #Manhood. http://t.co/vTP8i8QLEn"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 0 -- i miss my longer hair..but it was so dead anyways it wasn't even hair"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 0 -- @Babybackreeve FATALITY!!!!!!!!!!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 1 -- Demolition Means Progress: Flint Michigan and the Fate of the American Metropolis Highsmith https://t.co/ZvoBMDxHGP"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 1 -- How do you derail a train at... Smithsonian?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Target: 0 -- @paddytomlinson1 ARMAGEDDON"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [677, 2643, 3134, 92, 2290, 2062, 3681, 2343, 2384, 323]:\n",
    "    # Print using markdown for better formatting\n",
    "    tg = tweet.iloc[i][\"target\"]\n",
    "    tw = tweet.iloc[i][\"text\"]\n",
    "    display(Markdown(f\"Target: {tg} -- {tw}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that\n",
    "- There are many unusual symbols, such as in \"MenÛªs\", and hashtags (#)\n",
    "- We need to remove urls\n",
    "- Many tweets have date tags, such as \"8/6/2015@2:09 PM:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords\n",
    "The keyword column contain an important keyword present in the tweet, such as \"sinking\", as presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>Do you feel like you are sinking in low self-i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>After a Few Years Afloat Pension Plans Start S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>Do you feel like you are sinking in unhappines...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>With a sinking music video tv career Brooke Ho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6089</th>\n",
       "      <td>@supernovalester I feel so bad for them. I can...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6090</th>\n",
       "      <td>#nowplaying Sinking Fast - Now or Never on Nor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091</th>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6092</th>\n",
       "      <td>Nigga car sinking but he snapping it up for fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6093</th>\n",
       "      <td>@abandonedpics You should delete this one it's...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6094</th>\n",
       "      <td>that horrible sinking feeling when youÛªve be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "6085  Do you feel like you are sinking in low self-i...       0\n",
       "6086  After a Few Years Afloat Pension Plans Start S...       1\n",
       "6087  Do you feel like you are sinking in unhappines...       0\n",
       "6088  With a sinking music video tv career Brooke Ho...       0\n",
       "6089  @supernovalester I feel so bad for them. I can...       0\n",
       "6090  #nowplaying Sinking Fast - Now or Never on Nor...       0\n",
       "6091  that horrible sinking feeling when youÛªve be...       1\n",
       "6092  Nigga car sinking but he snapping it up for fo...       0\n",
       "6093  @abandonedpics You should delete this one it's...       0\n",
       "6094  that horrible sinking feeling when youÛªve be...       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet[tweet['keyword']=='sinking'][[\"text\", \"target\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicates\n",
    "110 tweets in the dataset have/are duplicates. Some have contradictory labelling. We drop the duplicates, as there are few compared to the data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(tweet.duplicated(subset=['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet.drop_duplicates(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dataset balance\n",
    "The dataset appears to be fairly balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_dist = tweet.target.value_counts()\n",
    "\n",
    "sns.barplot(x=value_dist.index, y=value_dist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "We will try adding a sentiment analysis score to our tweets. `SentimentIntensityAnalyzer` from `nltk` gives pieces of text a sentiment score between -1 and 1, where 1 is very positive and -1 is very negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Add estimated sentiment to model input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "sia_table = []\n",
    "for tweet_i in tweet['text']:\n",
    "    sia_table.append(sia.polarity_scores(tweet_i)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet['sentiment'] = sia_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We inspect ten random tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target | Sentiment | Tweet\n",
      "     1 |    0.0000 | http://t.co/iXiYBAp8Qa The Latest: More homes razed by Northern California wildfire - Lynchburg News and Advance http://t.co/zEpzQYDby4\n",
      "     0 |    0.6636 | Reading for work has collided with reading for pleasure. Huzzah. Don't miss @molly_the_tanz's Vermilion! http://t.co/83bMprwH7W\n",
      "     0 |    0.0000 | think i'll become a businessman a demolish a community centre and build condos on it but foiled by a troupe of multi-racial breakdancers .\n",
      "     1 |   -0.8481 | 70 Years After Atomic Bombs Japan Still Struggles With War Past: The anniversary of the devastation wrought b... http://t.co/LtVVPfLSg8\n",
      "     0 |    0.0000 | 'Snowstorm' 36'x36' oil on canvas (2009) http://t.co/RCZAlRU05o #art #painting\n",
      "     0 |    0.0000 | We would become the mirrors that reflected each other's most private wounds and desires.\n",
      "     1 |   -0.6369 | #Colorado #News Motorcyclist bicyclist injured in Denver collision on Broadway: At least two people were tak... http://t.co/2iAFPmqJeP\n",
      "     1 |   -0.6249 | Families to sue over Legionnaires: More than 40 families affected by the fatal outbreak of Legionnaires' disea... http://t.co/NQ77EfMF88\n",
      "     1 |    0.0000 | VIDEO: 'We're picking up bodies from water': Rescuers are searching for hundreds of migrants in the Mediterran... http://t.co/ZFWMjh6SLh\n",
      "     0 |   -0.6808 | @punkblunts @sincerelyevelnn fall off a cliff into hell idc\n"
     ]
    }
   ],
   "source": [
    "print(f\"Target | Sentiment | Tweet\")\n",
    "for i, row in tweet.iloc[[5585, 1708, 2297, 2759, 6229, 7439, 1766, 5314, 5725, 1582]].iterrows():\n",
    "    print(f\"{row.target:6} | {row.sentiment:9.4f} | {row.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x=tweet.target, y=tweet.sentiment)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment of disaster tweets seem to fall slightly lower than non-disaster tweets on average. It might have stronger predictive quality together with high leverl features of the tweets discovered by the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment does not seem to separate the two classes, but might be predictive in connection with higher level features detected by the neural network.\n",
    "- TODO: How to implement sentiment in the analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngram analysis\n",
    "- Uninformative\n",
    "- From https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove#Exploratory-Data-Analysis-of-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tweet_bigrams(corpus, n=None):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding vectorization\n",
    "We will vectorize words using a library of vectors from a pre trained model.\n",
    "\n",
    "- TODO: Fine tune on current dataset\n",
    "  - see demo.sh in the github repo\n",
    "- Alternative dataset: https://allennlp.org/elmo\n",
    "\n",
    "\n",
    "We will use GloVe for vectorization of words found at https://github.com/stanfordnlp/GloVe,\n",
    "trying the twitter dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mmap\n",
    "\n",
    "def get_num_lines(file_path: str):\n",
    "    \"\"\"\n",
    "    Get the number of lines in a file. \n",
    "    Used in the tqdm module to get a progress bar for \n",
    "    for-loops when iterating over lines in a file.\n",
    "    \"\"\"\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this once to save an embedding dictionary to a file in and obj/ folder in current dir (`obj` dir must be created beforehand)\n",
    "\n",
    "embedding_dict={}\n",
    "file_path = './input/glove.twitter.27B.100d.txt'\n",
    "with open(file_path,'r') as f:\n",
    "    for line in tqdm(f, total = get_num_lines(file_path)):\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()\n",
    "\n",
    "save_obj(embedding_dict, \"embedding_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved embedding dictionary from previous cell\n",
    "embedding_dict = load_obj(\"embedding_dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO: Visualize embeddings with PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to preprocess the data based on how words are embedded into the pre-trained embeddings.\n",
    "\n",
    "We will first attempt some boiler-plate text cleaning, courtesy of among others [this notebook](https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove#GloVe-for-Vectorization), and inspect how well it coincides with the embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inital cleaning attempt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mortal Kombat X: All Fatalities On Meat Predator.\\nhttps://t.co/IggFNBIxt5'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.text.iloc[3639]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame):\n",
    "    import string\n",
    "    import re\n",
    "\n",
    "    punctuation_regex = re.compile(f\"[{re.escape(string.punctuation)}:]\")\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"\\U00002702-\\U000027B0\"  # Symbols\n",
    "                               \"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    # Clean training data\n",
    "    df.keyword.str.replace(\"%20\", \" \")  # We replace ascii %20 with space in the keywords, e.g 'body%20bags' -> 'body bags'\n",
    "    df.text = df.text.str.lower()\n",
    "    df.text = df.text.str.replace(r\"https[^\\s|$]*\", \"\", regex=True)  # Change all urls to \"URL\"\n",
    "    df.text = df.text.str.replace(punctuation_regex, \"\", regex=True)\n",
    "    df.text = df.text.str.replace(emoji_pattern, \"\", regex=True)  # Remove emojis and symbols\n",
    "    df.text = df.text.str.replace(r\"\\n\", \" \", regex = True)       # Change \\n to space\n",
    "    df.text = df.text.str.replace(r\"[^a-zA-Z0-9 ]\", \"\", regex=True)  # Remove last non word characters\n",
    "    df.text = df.text.str.replace(r\"<.*?>\", \"\", regex=True)  # Remove html tags (e.g. <div> )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweet = clean_data(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mortal kombat x all fatalities on meat predator '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_tweet.text.iloc[3639]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check embedding coverage\n",
    "\n",
    "We check how many of the words in our training set tweets are covered by the embedding dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_representation(word_dict: dict , word_list: list):\n",
    "    \"\"\"Return the words from uq_words not contained in word_dict \n",
    "    and the number of words not covered.\"\"\"\n",
    "    n_covered = 0\n",
    "    not_covered = []\n",
    "    for word in word_list:\n",
    "        if word in word_dict:\n",
    "            n_covered += 1\n",
    "        else:\n",
    "            not_covered.append(word)\n",
    "\n",
    "    return n_covered, not_covered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unused_words(word_dict: dict , word_list: list):\n",
    "    \"\"\"Returns a list of words from word_dict not contained in word_list\"\"\"\n",
    "    unused_words = word_dict.copy()#.keys())\n",
    "\n",
    "    # Remove words in tweet data from the dict\n",
    "    for word in word_list:\n",
    "        try:\n",
    "            unused_words.pop(word)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    #Convert dict to list\n",
    "    return list(unused_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_words = tweet.text.str.split(expand=True).stack().unique()\n",
    "n_covered, not_covered = word_representation(word_dict = embedding_dict, word_list = uq_words)\n",
    "unused_words = get_unused_words(word_dict = embedding_dict, word_list = uq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the coverage of the tweets, we see that only 56% of the words in our data are in the embedding dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5676696863477034"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_covered/len(uq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While only 1% of the words in the embedding dictionary is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01044813010309883"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_covered/len(embedding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calls for further inquiry. Checking the words not covered by the embeddings, we see that there are numbers, URL's and  words with repeated number of letters (elongated words) eg. 'goooooooaaaaaal' among other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13000,  rockyfire,  20,  cafire,  18,  19,  80,  goooooooaaaaaal,  bbcmtd,  httptcolhyxeohy6c,  httptcoyao1e0xngw,  africanbaze,  newsnigeria,  httptco2nndbgwyei,  httptcoqqsmshaj3n,  phdsquares,  httptco3imaomknna,  superintende,  httptcowdueaj8q4j,  httptcoroi2nsmejj'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",  \".join(not_covered[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When inspecting some of the unused words in the embedding dictionary we see that many things, such as hashtags, repeated letters in words, allcaps words and smileys are encoded with special placeholders, such as <allcaps>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<user>  .  :  ,  <repeat>  <hashtag>  <number>  <url>  !  \"  ?  (  <allcaps>  <elong>  )  <smile>  ！  。  -  、  …  /  \\'s  *  n\\'t  \\'  \\'m  >  ^  ？  <  ・  &  ♥  lo  “  ”  por  _  <sadface>  من  ♡  ´  ،  ~  ;  <heart>  aku  \\'re  <lolface>  una  （  >>  في  ･  le  é  |  [  ）  ]  yg  —  笑  ω  je  yang  ❤  não  ～  ★  `  dia  $  و  الله  pero  ♪  \\'ll  =  nn  ｀  ¿  <neutralface>  لا  +  ada  ☆  ni  \\'ve  itu  على  -_-  ☺  ما  todo  mais  ini  ﾟ  aja'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"  \".join(unused_words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and pre-processing for the GloVe embedding\n",
    "In their [info page](https://nlp.stanford.edu/projects/glove/) the writers of the GloVe algorithm supply a ruby regex used for text pre-processing for the twitter model.\n",
    "\n",
    "In an [issue thread](https://github.com/stanfordnlp/GloVe/issues/107) discussing text pre-processing for tweets on their Github page user [skondrashov](https://github.co/skondrashov) supplies a useful python conversion of this ruby script, that also illustrates how words are adjusted to fit in a standard dictionary, and tagged for special characters or rewritings, such as being prefixed with a hashtag or elongated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove contractions\n",
    "Copied from https://www.analyticsvidhya.com/blog/2020/04/beginners-guide-exploratory-data-analysis-text-data/, with a small tweak for not matching `'s` in words surrounded by single quotation mark, like `'sylvester stallone'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r\"(ain't|(?<=[a-zA-Z])'s|aren't|can't|can't've|'cause|could've|couldn't|couldn't've|didn't|doesn't|don't|hadn't|hadn't've|hasn't|haven't|he'd|he'd've|he'll|he'll've|how'd|how'd'y|how'll|i'd|i'd've|i'll|i'll've|i'm|i've|isn't|it'd|it'd've|it'll|it'll've|let's|ma'am|mayn't|might've|mightn't|mightn't've|must've|mustn't|mustn't've|needn't|needn't've|o'clock|oughtn't|oughtn't've|shan't|sha'n't|shan't've|she'd|she'd've|she'll|she'll've|should've|shouldn't|shouldn't've|so've|that'd|that'd've|there'd|there'd've|they'd|they'd've|they'll|they'll've|they're|they've|to've|wasn't|we'd|we'd've|we'll|we'll've|we're|we've|weren't|what'll|what'll've|what're|what've|when've|where'd|where've|who'll|who'll've|who've|why've|will've|won't|won't've|would've|wouldn't|wouldn't've|y'all|y'all'd|y'all'd've|y'all're|y'all've|you'd|you'd've|you'll|you'll've|you're|you've)\",\n",
       "re.UNICODE)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Dictionary of English Contractions\n",
    "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"i'd\": \"i would\", \"i'd've\": \"i would have\",\"i'll\": \"i will\",\n",
    "                     \"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\"}\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "#    adding positive lookbehind for `'s` in the regex to make sure a letter is preceeding\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()).lower().replace(\"|'s\", \"|(?<=[a-zA-Z])'s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet= pd.read_csv('./input/nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('./input/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(1)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "# Expanding Contractions in the reviews\n",
    "df = tweet.copy()\n",
    "df.loc[:, 'text']=df.loc[:, 'text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's an emergency evacuation happening now in the building across the street\n",
      "There is an emergency evacuation happening now in the building across the street\n",
      "What's up man?\n",
      "What is up man?\n",
      "No way...I can't eat that shit\n",
      "No way...I cannot eat that shit\n",
      "@PhDSquares #mufc they've built so much hype around new acquisitions but I doubt they will set the EPL ablaze this season.\n",
      "@PhDSquares #mufc they have built so much hype around new acquisitions but I doubt they will set the EPL ablaze this season.\n",
      "on the outside you're ablaze and alive\n",
      "but you're dead inside\n",
      "on the outside you are ablaze and alive\n",
      "but you are dead inside\n",
      "First night with retainers in. It's quite weird. Better get used to it; I have to wear them every single night for the next year at least.\n",
      "First night with retainers in. It is quite weird. Better get used to it; I have to wear them every single night for the next year at least.\n",
      "@ablaze what time does your talk go until? I don't know if I can make it due to work.\n",
      "@ablaze what time does your talk go until? I do not know if I can make it due to work.\n",
      "'I can't have kids cuz I got in a bicycle accident &amp; split my testicles. it's impossible for me to have kids' MICHAEL YOU ARE THE FATHER\n",
      "'I cannot have kids cuz I got in a bicycle accident &amp; split my testicles. it is impossible for me to have kids' MICHAEL YOU ARE THE FATHER\n",
      "mom: 'we didn't get home as fast as we wished' \n",
      "me: 'why is that?'\n",
      "mom: 'there was an accident and some truck spilt mayonnaise all over ??????\n",
      "mom: 'we did not get home as fast as we wished' \n",
      "me: 'why is that?'\n",
      "mom: 'there was an accident and some truck spilt mayonnaise all over ??????\n",
      "#TruckCrash Overturns On #FortWorth Interstate http://t.co/Rs22LJ4qFp Click here if you've been in a crash&gt;http://t.co/Ld0unIYw4k\n",
      "#TruckCrash Overturns On #FortWorth Interstate http://t.co/Rs22LJ4qFp Click here if you have been in a crash&gt;http://t.co/Ld0unIYw4k\n"
     ]
    }
   ],
   "source": [
    "# Print the ten first edited tweets\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "while j < 5:\n",
    "    a = tweet.loc[i,'text']\n",
    "    b = df.loc[i,'text']\n",
    "    if (len(a) != len(b)):\n",
    "        print(a)\n",
    "        print(b)\n",
    "        j += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning function for repeated letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g', 'o', 'oo', ''), ('', 'a', 'aaa', ''), ('', 'l', 'll', '')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: \n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "word = \"goooaaaallls\"  # We want to match go[oo]a[aaa]l[ll]s\n",
    "\n",
    "# Get list of matches: [(group1, group2, ...), ...] where match group 3 is the repeated letters\n",
    "m = re.findall(r\"(\\S*?)(\\w)(\\2{1,})(\\S*?)\", word)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oo', 'aaa', 'll']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repeated_letters = [match[2] for match in m]\n",
    "repeated_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned word - Removed letters\n",
      "=============================\n",
      "goals         ('oo', 'aaa', 'll')\n",
      "goallls       ('oo', 'aaa')\n",
      "goaaaals      ('oo', 'll')\n",
      "goooals       ('aaa', 'll')\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Loop over all combinations of repeated letters\n",
    "print(f\"{'Cleaned word':12s} - Removed letters\")\n",
    "print(\"=============================\")\n",
    "for i in range(len(repeated_letters), 0, -1):\n",
    "    for combination in combinations(repeated_letters, r=i+1):\n",
    "        tword = word\n",
    "        for letters in combination:\n",
    "            tword = re.sub(letters, \"\", tword)\n",
    "        print(f\"{tword:14s}{combination}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning function for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def clean_tweets(df):\n",
    "    import re\n",
    "\n",
    "    def sub(pattern, output, string, whole_word=False):\n",
    "        token = output\n",
    "        if whole_word:\n",
    "            pattern = r'(\\s|^)' + pattern + r'(\\s|$)'\n",
    "\n",
    "        if isinstance(output, str):\n",
    "            token = ' ' + output + ' '\n",
    "        else:\n",
    "            token = lambda match: ' ' + output(match) + ' '\n",
    "\n",
    "        return re.sub(pattern, token, string)\n",
    "\n",
    "\n",
    "    def hashtag(token):\n",
    "        \"\"\" Replace hashtag `#` with `<hashtag>` and split following joined words.\"\"\"\n",
    "        token = token.group('tag')\n",
    "        if token != token.upper():\n",
    "            token = ' '.join(re.findall('[a-zA-Z][^A-Z]*', token))\n",
    "\n",
    "        return '<hashtag> ' + token\n",
    "\n",
    "    def punc_repeat(token):\n",
    "        return token.group(0)[0] + \" <repeat>\"\n",
    "\n",
    "    def punc_separate(token):\n",
    "        return token.group()\n",
    "\n",
    "    def number(token):\n",
    "        return token.group() + ' <number>';\n",
    "\n",
    "    def word_end_repeat(token):\n",
    "        return token.group(1) + token.group(2) + ' <elong>'\n",
    "    \n",
    "    def allcaps(token):\n",
    "        return token.group() + ' <allcaps>'\n",
    "\n",
    "    def clean_repeated_letters(tweet: str, embedding_dict: dict):\n",
    "        \"\"\"\n",
    "        Splits a tweet into words, finds repeated letters in the word and\n",
    "        removes combinations of the repeated letters until the word is matched by a key in\n",
    "        embedding_dict\n",
    "        \"\"\"\n",
    "\n",
    "        cleaned_tweet = []\n",
    "\n",
    "        for word_i in tweet.split():\n",
    "            word_found = False\n",
    "            if word_i in embedding_dict:\n",
    "                cleaned_tweet.append(word_i)\n",
    "                continue\n",
    "\n",
    "            matches = re.findall(r\"\"\"(\\S*?)    # 1: Optional preceeding letters\n",
    "                                     (\\w)      # 2: A letter that might be repeated\n",
    "                                     (\\2{1,})  # 3: Repetead instances of the preceeding letter (group 2)\n",
    "                                     (\\S*?)    # 4: Optional trailing letters\"\"\",\n",
    "                                 word_i,\n",
    "                                 flags=re.X)  # Verbose regex, for commenting\n",
    "                                 \n",
    "            repeated_letters = [match[2] for match in matches]\n",
    "                    \n",
    "            # Loop over all combinations of repeated letters\n",
    "            for i in range(len(repeated_letters), 0, -1):  # i decides length of combination\n",
    "                if word_found:\n",
    "                    continue\n",
    "                    \n",
    "                for combination in combinations(repeated_letters, r = i):\n",
    "                    if word_found:\n",
    "                        continue\n",
    "                        \n",
    "                    tword = word_i \n",
    "                    \n",
    "                        \n",
    "                    for letters in combination:\n",
    "                        tword = re.sub(letters, \"\", tword, count=1)\n",
    "                                        \n",
    "                        # Word in the embedding dict?\n",
    "                        if (tword in embedding_dict):\n",
    "                            # Keep the word and stop searching\n",
    "                            word_found = True\n",
    "                            tword = tword + \" <elong>\"\n",
    "                            continue  \n",
    "            if not word_found:\n",
    "                # No match, we simply keep the word\n",
    "                tword = word_i\n",
    "                \n",
    "            cleaned_tweet.append(tword)\n",
    "            \n",
    "        return \" \".join(cleaned_tweet)\n",
    "\n",
    "\n",
    "\n",
    "    eyes        = r\"[8:=;]\"\n",
    "    nose        = r\"['`\\-\\^]?\"\n",
    "    sad_front   = r\"[(\\[/\\\\]+\"\n",
    "    sad_back    = r\"[)\\]/\\\\]+\"\n",
    "    smile_front = r\"[)\\]]+\"\n",
    "    smile_back  = r\"[(\\[]+\"\n",
    "    lol_front   = r\"[DbpP]+\"\n",
    "    lol_back    = r\"[d]+\"\n",
    "    neutral     = r\"[|]+\"\n",
    "    sadface     = eyes + nose + sad_front   + '|' + sad_back   + nose + eyes\n",
    "    smile       = eyes + nose + smile_front + '|' + smile_back + nose + eyes\n",
    "    lolface     = eyes + nose + lol_front   + '|' + lol_back   + nose + eyes\n",
    "    neutralface = eyes + nose + neutral     + '|' + neutral    + nose + eyes\n",
    "    punctuation = r\"\"\"[ '!\"#$%&'()+,/:;=?@_`{|}~\\*\\-\\.\\^\\\\\\[\\]]+\"\"\" ## < and > omitted to avoid messing up tokens\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        df.loc[i,'text'] = sub(r'[\\s]+',                             '  ',            df.loc[i,'text']) # ensure 2 spaces between everything\n",
    "        df.loc[i,'text'] = sub(r'(?:(?:https?|ftp)://|www\\.)[^\\s]+', '<url>',         df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(r'@\\w+',                              '<user>',        df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(r'#(?P<tag>\\w+)',                     hashtag,         df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(sadface,                              '<sadface>',     df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(smile,                                '<smile>',       df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(lolface,                              '<lolface>',     df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(neutralface,                          '<neutralface>', df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(r'(?:<3+)+',                          '<heart>',       df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(r'\\b[A-Z]+\\b',                         allcaps,       df.loc[i,'text'], True) \n",
    "        # Allcaps tag\n",
    "        df.loc[i,'text'] = df.loc[i,'text'].lower()\n",
    "        df.loc[i,'text'] = expand_contractions(df.loc[i, 'text'])\n",
    "        df.loc[i,'text'] = sub(r'[-+]?[.\\d]*[\\d]+[:,.\\d]*',          number,          df.loc[i,'text'], True)\n",
    "        df.loc[i,'text'] = sub(punctuation,                          punc_separate,   df.loc[i,'text'])\n",
    "        df.loc[i,'text'] = sub(r'([!?.])\\1+',                        punc_repeat,     df.loc[i,'text'])\n",
    "#     df.loc[i,'text'] = sub(r'(\\S*?)(\\w)\\2+\\b',                   word_end_repeat, df.loc[i,'text'])\n",
    "        \n",
    "        df.loc[i,'text'] = clean_repeated_letters(df.loc[i,'text'], embedding_dict)\n",
    "#     tweet = sub(r\"(\\S*?)(\\w)(\\2{1,})(\\S*?)\",          word_repeat,     tweet)\n",
    "#     tweet = sub(r'(\\S*?)(\\w*(\\w)\\2+\\w*)\\2+\\b',                   word_repeat, tweet)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the cleaning function\n",
    "We test the cleaning on some text to see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  I'm hoping they're helping, they've got to\n",
      "Cleaned:   i am hoping they are helping , they have got to\n",
      "Original:  goooooooaaaaaallll, hey its a goall gooal\n",
      "Cleaned:   goal <elong> , hey its a goall gooal\n",
      "Original:  http://foo.com/blah_blah http://foo.com/blah_blah/ http://foo.com/blah_blah_(wikipedia) https://foo_bar.example.com/\n",
      "Cleaned:   <url> <url> <url> <url>\n",
      "Original:  :\\ :-/ =-( =`( )'8 ]^; -.- :/\n",
      "Cleaned:   <sadface> <sadface> <sadface> <sadface> <sadface> <sadface> -.- <sadface>\n",
      "Original:  :) :-] =`) ('8 ;`)\n",
      "Cleaned:   <smile> <smile> <smile> <smile> <smile>\n",
      "Original:  :D :-D =`b d'8 ;`P\n",
      "Cleaned:   <lolface> <lolface> <lolface> <lolface> <lolface>\n",
      "Original:  :| 8|\n",
      "Cleaned:   <neutralface> <neutralface>\n",
      "Original:  <3<3 <3 <3\n",
      "Cleaned:   <heart> <heart> <heart>\n",
      "Original:  #swag #swa00-= #as ## #WOOP #Feeling_Blessed #helloWorld\n",
      "Cleaned:   <hashtag> swag # swa00 -= <hashtag> as ## <hashtag> woop <allcaps> <hashtag> feeling _ blessed <hashtag> hello world\n",
      "Original:  holy crap!! i won!!!!@@!!!\n",
      "Cleaned:   holy crap ! <repeat> i won ! <repeat> @@ ! <repeat>\n",
      "Original:  holy *IUYT$)(crap!! @@#i@%#@ swag.lord **won!!!!@@!!! wahoo....!!!??!??? Im sick lol.\n",
      "Cleaned:   holy * iuyt $)( crap ! <repeat> @@# i @%#@ swag . lord ** won ! <repeat> @@ ! <repeat> wahoo . <repeat> ! <repeat> ? <repeat> ! ? <repeat> im sick lol .\n",
      "Original:  this SENTENCE consisTS OF slAyYyyy slayyyyyy #WEIRD caPITalIZAtionn\n",
      "Cleaned:   this sentence <allcaps> consists of <allcaps> slay <elong> slay <elong> <hashtag> weird <allcaps> capitalization <elong>\n"
     ]
    }
   ],
   "source": [
    "temp = pd.DataFrame({\"text\": [\n",
    "    u\"I'm hoping they're helping, they've got to\",\n",
    "    u'goooooooaaaaaallll, hey its a goall gooal',\n",
    "    u'http://foo.com/blah_blah http://foo.com/blah_blah/ http://foo.com/blah_blah_(wikipedia) https://foo_bar.example.com/',\n",
    "    u':\\\\ :-/ =-( =`( )\\'8 ]^; -.- :/',\n",
    "    u':) :-] =`) (\\'8 ;`)',\n",
    "    u':D :-D =`b d\\'8 ;`P',\n",
    "    u':| 8|',\n",
    "    u'<3<3 <3 <3',\n",
    "    u'#swag #swa00-= #as ## #WOOP #Feeling_Blessed #helloWorld',\n",
    "    u'holy crap!! i won!!!!@@!!!',\n",
    "    u'holy *IUYT$)(crap!! @@#i@%#@ swag.lord **won!!!!@@!!! wahoo....!!!??!??? Im sick lol.',\n",
    "    u'this SENTENCE consisTS OF slAyYyyy slayyyyyy #WEIRD caPITalIZAtionn',\n",
    "    ]})\n",
    "temp_uncleaned = temp.copy()\n",
    "clean_tweets(df = temp)\n",
    "\n",
    "for i in range(temp.shape[0]):\n",
    "#     print(\"====================\")\n",
    "    print(\"Original: \", temp_uncleaned.iloc[i].text)\n",
    "    print(\"Cleaned:  \", temp.iloc[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Cleaning and checking embedding coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet= pd.read_csv('./input/nlp-getting-started/train.csv')\n",
    "test=pd.read_csv('./input/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_words = tweet.text.str.split(expand=True).stack().unique()\n",
    "n_covered, not_covered = word_representation(word_dict = embedding_dict, word_list = uq_words)\n",
    "unused_words = get_unused_words(word_dict = embedding_dict, word_list = uq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now 82% of the words in our data are in the embedding dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8229903115998952"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_covered/len(uq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still only 1% of the words in the embedding dictionary is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010533591953180313"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_covered/len(embedding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words not covered by the embedding dictionary seem to be joined words such as `myreligion` many uncommon symbols and words and numbers. Hopefully these do not carry much meaning, and as they are uncommon it will be hard for our model to descipher their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"13  000  20  18  19  80  africanbaze  \\x89ûò  \\x89ûó  superintende  ancop  3  lanford  carolinaåêablaze  '@  diyala  r21  voortrekker  \\x89û  gtxrwm  2k13  30  2013  tinderbox  24  nashvilletraffic  8m  101  personalinjury  caraccidentlawyer  tee\\x89û  bigrigradio  77  31  6  1  sleepjunkies  08  06  15  11  03  58  40  ?'  ://  ld0uniyw4k  23  752  540  999  horndale  naayf  chandanee  conf\\x89û  293  3a  4  1600  17th  2015  2  09  2781  suffield  9  langtree  115  150  16  5  your4state  marinading  .@  norwaymfa  arrestpastornganga  .'  320  icemoon  ices\\x89û  xb1  wiedemer  \\x89ã¢  2010  full\\x89ã¢  ;&  wdyouth  ps4  8015  29  07  wednesday\\x89û  t48  mbataweel  freaky\\x89û  airplaneåê  ladins  \\x89ûïairplane\\x89û\\x9d  can\\x89ûªt  g90\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words  in our data not in the embedding dictionary\n",
    "\"  \".join(not_covered[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Unused words from the embedding dictionary are mainly symbols and foreign words. Which means we seem to have captured most of the important meaning-bearing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"  ！  。  、  …  \\'s  n\\'t  \\'m  ？  <  ・  ♥  “  ”  por  من  ♡  ´  ،  <heart>  aku  \\'re  una  （  >>  في  ･  le  é  ）  yg  —  笑  ω  je  yang  ❤  não  ～  ★  dia  و  الله  pero  ♪  \\'ll  nn  ｀  ¿  لا  ada  ☆  ni  \\'ve  على  ☺  ما  todo  mais  ini  ﾟ  aja  ▽  apa  cuando  ✔  ؟  •  quiero  kamu  nada  <<  ta  lagi  █  más  mau  ｡  pas  в  hoy  meu  gak  amor  \\\\  ver  ；  porque  اللهم  gue  uma  vou  bien  ¡  ＾  ／  todos  →  kan  kita'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unused words in the embedding dictionary\n",
    "\"  \".join(unused_words[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: text, dtype: object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.text[tweet.text.str.match(r\".*jonvoyage\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happenning\n",
      "{'happenning', 'hapening'}\n"
     ]
    }
   ],
   "source": [
    "spell = SpellChecker()\n",
    "\n",
    "misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n",
    "\n",
    "    # Get a list of `likely` options\n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work nicely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation copied from https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO implement into the data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Remove non words\n",
    "- encode urls?\n",
    "- Encode complexity of text: https://pypi.org/project/textstat/\n",
    "- Removal of stop words? (library for specific language)\n",
    "- Remove URL's\n",
    "- Spelling/grammar correction?\n",
    "  - Companies like Google and Microsoft have achieved a decent accuracy level in automated spell correction. One can use algorithms like the Levenshtein Distances, Dictionary Lookup etc. or other modules and packages to fix these errors.\n",
    "  - Number of misspelled words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords\n",
    "We can use NLTK to remove common words.\n",
    "These words contain little information on their own, but they might convey information in the sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/jonaso/nltk_data'\n    - '/home/jonaso/anaconda3/nltk_data'\n    - '/home/jonaso/anaconda3/share/nltk_data'\n    - '/home/jonaso/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/home/jonaso/nltk_data'\n    - '/home/jonaso/anaconda3/nltk_data'\n    - '/home/jonaso/anaconda3/share/nltk_data'\n    - '/home/jonaso/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3181/3054741396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/home/jonaso/nltk_data'\n    - '/home/jonaso/anaconda3/nltk_data'\n    - '/home/jonaso/anaconda3/share/nltk_data'\n    - '/home/jonaso/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
